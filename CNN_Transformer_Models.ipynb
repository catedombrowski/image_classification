{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Using CNN on MNIST Data**"
      ],
      "metadata": {
        "id": "oV9j-fRMtHo5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQo-5Z-Vsvqs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel:\n",
        "    def __init__(self):\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Flatten(),\n",
        "\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),   # Adding dropout with a dropout rate of 0.5\n",
        "        layers.Dense(10, activation='softmax')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def compile_model(self):\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.model.compile(optimizer=optimizer,\n",
        "                       loss='sparse_categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "    def train_model(self, train_images, train_labels, epochs=5, batch_size=64, validation_split=0.1):\n",
        "        history = self.model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
        "        return history\n",
        "\n",
        "    def evaluate_model(self, test_images, test_labels):\n",
        "        test_loss, test_acc = self.model.evaluate(test_images, test_labels)\n",
        "        print('Test accuracy:', test_acc)\n",
        "        return test_loss, test_acc\n",
        "\n",
        "    def plot_history(self, history):\n",
        "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "PmIMMGDks_LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load and preprocess the MNIST dataset\n",
        "    (train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
        "    train_images = train_images.reshape((train_images.shape[0], 28, 28, 1)).astype('float32') / 255.0\n",
        "    test_images = test_images.reshape((test_images.shape[0], 28, 28, 1)).astype('float32') / 255.0\n",
        "\n",
        "    # Create an instance of CNNModel\n",
        "    cnn_model = CNNModel()\n",
        "\n",
        "    # Compile the model\n",
        "    cnn_model.compile_model()\n",
        "\n",
        "    # Train the model\n",
        "    history = cnn_model.train_model(train_images, train_labels)\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_acc = cnn_model.evaluate_model(test_images, test_labels)\n",
        "\n",
        "    # Plot training and validation accuracy\n",
        "    cnn_model.plot_history(history)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "5uV8TU8ttBUr",
        "outputId": "b0f28e28-2fd7-4817-c297-9ccac44ca5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "844/844 [==============================] - 11s 7ms/step - loss: 0.2102 - accuracy: 0.9360 - val_loss: 0.0420 - val_accuracy: 0.9870\n",
            "Epoch 2/5\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 0.0681 - accuracy: 0.9807 - val_loss: 0.0315 - val_accuracy: 0.9915\n",
            "Epoch 3/5\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 0.0475 - accuracy: 0.9864 - val_loss: 0.0343 - val_accuracy: 0.9913\n",
            "Epoch 4/5\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 0.0369 - accuracy: 0.9890 - val_loss: 0.0324 - val_accuracy: 0.9923\n",
            "Epoch 5/5\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 0.0298 - accuracy: 0.9910 - val_loss: 0.0251 - val_accuracy: 0.9937\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9936\n",
            "Test accuracy: 0.9936000108718872\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABgsElEQVR4nO3deXxM9/4/8NfMJDOTfZE9skiEWBMiIrSl6I2lLqotaolodbloXde3pRTVhbaqFD/tdQml1lpub3vL1SgldhG0xC6JyCKW7JkkM+f3xyTDyCKT7WRmXs/HYx4yZz5z5v3JJOaVz/l8zpEIgiCAiIiIyIxIxS6AiIiIqKkxABEREZHZYQAiIiIis8MARERERGaHAYiIiIjMDgMQERERmR0GICIiIjI7FmIX0BxpNBrcvn0bdnZ2kEgkYpdDREREtSAIAvLy8uDl5QWptOYxHgagKty+fRs+Pj5il0FERER1kJqaipYtW9bYhgGoCnZ2dgC030B7e3uRqyEiIqLayM3NhY+Pj+5zvCYMQFWoOOxlb2/PAERERGRkajN9hZOgiYiIyOwwABEREZHZYQAiIiIis8MARERERGaHAYiIiIjMDgMQERERmR0GICIiIjI7DEBERERkdhiAiIiIyOwwABEREZHZYQAiIiIis8MARERERGaHF0MlIiKipiEIQEkBUHQfkFkCdh6ilcIARERERIZRlwLFOdogU3QfKHrw8OviBzVsuw9oyrT7CB0DDPt/onWBAYiIiMgcCQJQkm9ggHmgvZXk1e+1pZaARl3fHtQLAxAREZExU5caNgJTsb34wcPRmLpSOABWjuU3J+1NWfF1VdvKt1taAxJJ/V67nhiAiIiIxCYIgCrPgADz4OG2kvz6vbZMXn1QqSnUKOwBmfHGCOOtnIiIqLkpKzFsBObRbUI9DwkpHaoOME8KNZZWoo/GiIEBiIiI6FGCAKhyaxlgHuhvKy2o32vLFE8+fFTxr/LRkOMASGX1e20zwwBERESmqUxV/WhLdduK7mtXN9VrNEYCKO1rDjDVjcpYWtW311RLDEBERNT8CII2wKjytCuOVPnauS6qfO3oTHFO1fNiHg02pYX1q0GmAKydawgwjlWP1nA0xigwABERUcPQaLSHgHRhJbdycKn4utpteeWhJ7/+K5QAaEdjHGp3WKnSSiWOxpgyBiAiInOmLn0YOKoKIar88hGYKkZhHg8zJfkAhIav0dIaUNgBcltAYQvI7R4LNY7VhxqFAyDlVZ+oMgYgIiJjIghAWXE1weTx4FKLMFNW3PA1SqTakKKwfSy42GqXTuu+Lg8zCrtHttlVfg4PJ1EjYAAiImpsGo02bOiFkPxHgskj/9YmzNR3uXRVZIqaQ4ghwcVMl1WTcWEAIqqt/Cwg5RiQdRGAoP0rVyIp/1dW/u8jN6nskcelVbeTSis/T6+N5JF9VdVO8tjrVbW/qup6bP811W+u9A4N5VUTXGoZZup7orrqyG0fCyl1DC5yW8BC3jg1EjVTDEBEVREEIPsKkHIUSD2u/ffedbGrEkd14e2JAc7AcFbrYFaL0PikwKhRPznMqFWN8L2UlR8Wsn8spFS17fEw81hwkdtybgtRPTAAEQHa5ba3Ex8JPMeAonuPNZIAbu0Ar67av5YFzcObRqN/X9BoD1MIGm2Y0rVTV9GuipumiucKVTy3rq9ryERV3XMaYkWOEbJQPhJS7B7Obal1cHlkFMZCad6jakTNCAMQmafCe0DqiYeBJy2h8l/8FkrAuxvgGwH4RgItw7UrS0yBXrCqKng9GqCqCl7qKvbxeDuhmsDXEMHwSfVV95rlN4m0ijBTVXCxA2SWYr9bREZJEATkFpchM7cYGTnFyHjk38ycYvQIaIFJzwSIVh8DEJk+QQDu3wBSjj8MPHeSKrezdgF8e2hvPj0AzxDTnRchkWgPx4Cra4jIcGVqDbLzSx6GmpwiZOSqdGEnM7cY6TnFKCqtfsK+3ELcQ7gMQGR61KVAxjn9wJOfWbldi6CHgcc3EnAO4OEJIjJ7hSVl5aGmfNSmfMTm0RGcO3kqaGp5JN1eaQEPByU8HKzgYa+Ah70S7g5KtHW3a9yOPAEDEBm/4lzg1omHgSftdOVT4EstAa8uDw9n+UQANi7i1EtEJAKNRsDdghK9Q1IVIzWPbssrrt18P5lUAjc7BdztlfCwV8LDQan92kEBD3ur8vsKWMubZ9QQvaqVK1fiiy++QEZGBkJCQrB8+XJ07969yralpaVYuHAh1q9fj7S0NLRt2xafffYZBgwYoGuTl5eHDz74ALt27UJWVha6dOmCZcuWITw8vKm6RI0t55Z2knLFLevP8km6j1A6aA9jVQQery48rT0RmaziUjWyclV6Iza6YFMebrLyilGqrt2wjY1cBneH8mBTHm50Aaf8voutAjKp8Y6aixqAtm7diunTp+Obb75BREQEli5diqioKFy6dAlubm6V2s+ZMwcbN27E6tWrERwcjL1792L48OE4cuQIunTpAgB47bXX8Mcff2DDhg3w8vLCxo0b0b9/f1y4cAHe3t5N3UWqL40ayLqgH3hyb1Vu5+inDToVgcelLZcIE5HREwQBOUWlj8y1eThyo/1ahYycItwvLK3V/iQSoIWN4pFRmvJDUuWhxrM85NgpTX/yv0QQhEa4cEvtREREIDw8HCtWrAAAaDQa+Pj4YOrUqZg5c2al9l5eXpg9ezYmT56s2zZixAhYWVlh48aNKCoqgp2dHf79739j8ODBujZhYWEYOHAgPv744yrrUKlUUKkergDKzc2Fj48PcnJyYG9v31DdpdooKdAewqoIO7dOaq859CiJDPDo9DDw+PQA7D3FqZeIqI5K1RrcyVPpHYKqGLF5dJuqTPPknQFQWEgrjdK42z8MNR4OSrjZKWApM90/DnNzc+Hg4FCrz2/RRoBKSkpw+vRpzJo1S7dNKpWif//+OHr0aJXPUalUUCqVetusrKxw+PBhAEBZWRnUanWNbaqycOFCfPjhh3XtCtVHXiaQ+sjoTsa5yleAlttql6BXBB7vbtplykREzVRecWl5gFHpjdg8elgqO1+F2g5BOFlbVhqlqZhMXHGYytHaEhIu5Kg10QJQdnY21Go13N3d9ba7u7sjKamKJcoAoqKisGTJEjzzzDMIDAxEXFwcdu7cCbVau8zOzs4OkZGR+Oijj9CuXTu4u7tj8+bNOHr0KFq3bl1tLbNmzcL06dN19ytGgKiBaTRA9mX9wHP/RuV2dl4PV2b5RgBuHQCZ6NPViIig1gi4m6/SHZKqmET8aMjJzFUhX1W7icQWUoku2Dw8FKUoH7mxgoe9Em72CigtecqKhmZUnyrLli3DpEmTEBwcDIlEgsDAQMTExGDt2rW6Nhs2bMDEiRPh7e0NmUyGrl27YvTo0Th9+nS1+1UoFFAoFE3RBfNSWgzcPvMw8KQeB4ruP9ZIArh30K7Kqgg8Dj5cjk5ETa64VF3phH0ZlSYSq6Cu5fpvO6WF/uqoR4JOxbYWNnJIjXgisTETLQC5uLhAJpMhM1P//CyZmZnw8PCo8jmurq7YvXs3iouLcffuXXh5eWHmzJkICHh4JsnAwEAcPHgQBQUFyM3NhaenJ0aOHKnXhhpJ4b2H181KOaYNP+oS/TYWVkDLbg8DT8tupnN2ZSJqlgRBwP3CUqTnFOkflsopRvoj57jJKardRGKpBHC1ezh52NNB/1BUxdc2CqMaYzA7or07crkcYWFhiIuLw7BhwwBoJ0HHxcVhypQpNT5XqVTC29sbpaWl2LFjB15++eVKbWxsbGBjY4P79+9j7969+PzzzxujG+ZLELQXB005Vj7CcxzIvlS5nY3rwzMr+0YCnp15aQEiajAlZRpk5uqP0uid2yZXe0iqpJYTia0sZXqThrUjN4pHTuSnhIutHBYmPJHYXIgaT6dPn47o6Gh069YN3bt3x9KlS1FQUICYmBgAwPjx4+Ht7Y2FCxcCAI4fP460tDSEhoYiLS0N8+fPh0ajwbvvvqvb5969eyEIAtq2bYurV6/i//7v/xAcHKzbJ9WRuhRIP1d+ZuXywFOQVbmdS5tHAk8Pnl2ZiOqluFSNW/eLkHqvECn3CpF8V/tvxWhOdn7Jk3dSzsVWrjd52PPRScTlYcdeacGJxGZC1AA0cuRI3LlzB3PnzkVGRgZCQ0OxZ88e3cTolJQUSB85l0txcTHmzJmD69evw9bWFoMGDcKGDRvg6Oioa5OTk4NZs2bh1q1bcHZ2xogRI/DJJ5/A0pKjDgYpzgFSTz68lMStU0BZkX4bmbz87MrlgccnArBpIU69RGSUBEF7duKUe4XakHO3EMnlYSf1XiEycoufuFJKLpPCvfx8NhWXW3B/bL6Nm51S9GtPUfMi6nmAmitDziNgMh6klq/MKg88mX8CeOxHw8qpfO5OeeDx6gJYKqvcHRFRhZIyDdIeFCH5bkGlkZzUe4UoKKn+gpmA9qzEvi1s4OdsDd8W1vBxtoa3o1J3uQUnLv+mckZxHiASkUatDTi6+TvHgNy0yu2cWulfHd2lDc+uTESVCIKAB4WleiM32pGcAqTeK8LtnKIaR3EkEsDTXgkfZ2v4tbCGr7M25Pg6W8OvhQ0DDjUKBiBzoMoH0k5p5+2kHtMe2irJ028jkQGeIfqBx8696v0RkdkpVWtw+0GR3shNxUhO6r1C5D3hvDfWctljwebh1y2drKCw4HluqGkxAJmivIyHJxpMPaadvCw8NsQstwN8uj8MPN5hgNxGnHqJqFnIKSzVhpp7BZVCzu0HRXjS6W887JW6kPP4SI6LrZyjONSsMAAZO41Gu/w85ejDEZ77Nyu3s2/5MOz49gDc2gNS/sVFZE7K1Bqk5xRXmoOjvV+A3OKaR3GUllL4Oj8MNhVzcrSjONY8WzEZFQYgY1NaDNxOeGSE5zhQ/OCxRhLAveMjh7MiAEde2oPIHOQWlyKl/LCU3pyce4VIu1+EsicM47jaKbTB5rHDVb7O1nC1U3AUh0wGA1BzV3BX/9pZ6YmVz65saa09hFVxKYmW4YDSQZRyiahxqTUC0nOKdMHm0ZGc5HuFeFBY89mM5RZS+DhZwa+FTaWQ09LJCtZyfiyQeeBPenOiO7vy0YeB5+6Vyu1s3fWvneXBsysTmZJ8VRlS9IJNAVLuaU8GeOt+IUrVNY/iuNjKHx6iemxFlZudgteeIgIDkLjKSoCMc/qBpzC7cjvXYP3A49SKZ1cmMmIajYCMXO1cnKpGcu4W1Hx2Y0uZBD5OVU829nW25jWoiGqBvyVNqTgHSD3xcMJy2imgrFi/jUwOeHUtn78TqV2pZe0sTr1EVGeFJWXagFMebB693bpXhBJ1zdemcraRPxy5eXQkp4U1POyVkHEUh6heGICaUuJmYM97+tusnB9OVPbtAXiG8uzKREZAoxFwJ1+lG7nRhp2C8q+LkJ2vqvH5FlIJWjpZVZpo7FN+s1fysDZRY2IAakoVFwetuFCobyTgEsTDWUTNVHGpWu/w1KO31HuFUD3hCuMOVpZ6J/yrGM3xcbaGp4OSVxQnEhEDUFPyCgXePiN2FURUThC0oziVQk7511l5NY/iyKQSeDkq4edsox9yWljDx8kaDtYcxSFqrhiAiMgsCIKAP2/nIu5iFs6n5SCl/GzHxaU1j+LYKSzg28K6ipEcG3g6KmHJURwio8QAREQmq0BVhsNXs/FbUhb2J2VVOaIjlQCeDlaVrk9Vcd/BihfiJDJFDEBEZFKS7xZgf3ngOX79nt5qK2u5DE+1dkHPwBZo5WoLX2dreDtaQW7BURwic8MARERGrVStwamb97E/KRP7k7Jw7U6B3uO+ztboG+yGvsFuiAhw5lXHiQgAAxARGaG7+SocuHQH+y9l4ffLd5D3yEU8ZVIJwv2dykOPOwJdbXgIi4gqYQAiomZPEARcSM/F/otZ2H8pC4mpDyA8cjUIZxs5+rRxRd92bng6yBUOVlx9RUQ1YwAiomapsKQM8VfvYn9SFn5LykJGrv5Z09t72mtHedq5IaSlI8+MTEQGYQAiomYj9V4hfruUhbiLWTh6/S5KHjnRoJWlDL1au6BvsBueDXaFp4OViJUSkbFjACIi0ZSpNTidfB/7L2Vh/8UsXMnK13u8pZOVbgJzj4AWUFpyAjMRNQwGICJqUvcLSnDgchb2J93BwUtZyH1sAnOYn5Mu9AS52XICMxE1CgYgImpUgiAgKSNPd26eMyn3oXlkArOjtWX5BGZ39A5y5eUjiKhJMAARUYMrKlHj6PVsxF3UTmC+naM/gTnYww59g93Qr50bQn2cOIGZiJocAxARNYi0B0W6FVvxV7P1rpSusJA+MoHZDd6OnMBMROJiACKiOlFrBJxJuY+48tCTlJGn97i3oxWeDXZF32A3RAa4wErOCcxE1HwwABFRrT0oLMHBy3ewPykLBy/fwYPCUt1jUgnQ1dcJfdtpJzC3dbfjBGYiarYYgIioWoIg4HJmvu7Q1qnke3oTmB2sLNG7jXaUp3cbVzjZyMUrlojIAAxARKSnuFSNo9fvai87kZSFtAdFeo+3cbdF32B39A12Q1dfR1jIeCV1IjI+DEBEhPSchxOYD1/NRnHpwwnMcgspega2QL9gN/Rp6wYfZ2sRKyUiahgMQERmSK0RkJj6AL8lZSEuKQsX03P1HvewV2rn8rR1Q8/WLWAt538VRGRa+L8akZnIKSrF75fv4LekLBy4fAf3Ckp0j0kkQBcfR/Rr545n27qhnScnMBORaWMAIjJRgiDg2p18xJXP5TmVfB/qR2Yw2ykt9CYwt7BViFgtEVHTYgAiMiHFpWocv3Gv/NBWJlLv6U9gbu1mi37lJyMM83OCJScwE5GZYgAiMnKZucW662zFX81GYYla95hcJkWP8gnMfYM5gZmIqAIDEJGR0WgEnL31cALzn7f1JzC72yu0l5xo64ZerV1go+CvORHR4/g/I5ERyC0uxeEr2ouLHrychex8/QnMIS0ddYe2OnjZcwIzEdETMAARNUOCIOB6doF2lOdiFk7evIeyRycwKyzwTBtXPBvshj5tXeHCCcxERAZhACJqJlRlapy4cU93QsKbdwv1Hg9wtdGN8oT7O3MCMxFRPTAAEYkoK68YB5LuIC4pE4evZKPgkQnMljIJegS0wLNttROY/V1sRKyUiMi0MAARNSGNRsD5tBztKM+lLJy7laP3uKudAs+2dUXfYHc8FeQCW05gJiJqFPzflaiR5avKcPjKHcRdzMJvl+4gO1+l93hISwc8G+yGfsHu6OBlD6mUE5iJiBobAxBRI7iZXYC48rk8x2/cRan64QRmG7lMbwKzm51SxEqJiMwTAxBRAygp0+DUzXu60HM9u0Dv8VYuNni2rRv6tdNOYJZbcAIzEZGYGICI6uhOngoHLmnn8vx+ORv5qjLdYxZSCSICnHUTmANcbUWslIiIHscARGSAwpIyrDl0A78mZeHcrQcQHh7ZgoutHH3auqFfsBueCnKBndJSvEKJiKhGDEBEBpi18zz+nXhbd7+Td8UEZjd08nbgBGYiIiMh+kSElStXwt/fH0qlEhEREThx4kS1bUtLS7FgwQIEBgZCqVQiJCQEe/bs0WujVqvxwQcfoFWrVrCyskJgYCA++ugjCI/+qU5UB7cfFOGnc+kAgPlD2uPE+/3wn6lPYfpzbRDi48jwQ0RkREQdAdq6dSumT5+Ob775BhEREVi6dCmioqJw6dIluLm5VWo/Z84cbNy4EatXr0ZwcDD27t2L4cOH48iRI+jSpQsA4LPPPsOqVauwfv16dOjQAadOnUJMTAwcHBzw9ttvN3UXyYRsOJYMtUZAjwBnTOjVSuxyiIioHiSCiEMjERERCA8Px4oVKwAAGo0GPj4+mDp1KmbOnFmpvZeXF2bPno3Jkyfrto0YMQJWVlbYuHEjAOD555+Hu7s71qxZU22bJ8nNzYWDgwNycnJgb29fny6SiSgqUSNyURweFJbi23FhiOrgIXZJRET0GEM+v0U7BFZSUoLTp0+jf//+D4uRStG/f38cPXq0yueoVCoolfrnTLGyssLhw4d193v27Im4uDhcvnwZAHD27FkcPnwYAwcOrLYWlUqF3NxcvRvRo3YnpuFBYSl8nK3Qv5272OUQEVE9iXYILDs7G2q1Gu7u+h8m7u7uSEpKqvI5UVFRWLJkCZ555hkEBgYiLi4OO3fuhFr98PpJM2fORG5uLoKDgyGTyaBWq/HJJ59gzJgx1daycOFCfPjhhw3TMTI5giAgNv4GACA60h8yzvUhIjJ6ok+CNsSyZcsQFBSE4OBgyOVyTJkyBTExMZBKH3Zj27Zt+P7777Fp0yYkJCRg/fr1WLx4MdavX1/tfmfNmoWcnBzdLTU1tSm6Q0biyLW7uJyZD2u5DC918xG7HCIiagCijQC5uLhAJpMhMzNTb3tmZiY8PKqeX+Hq6ordu3ejuLgYd+/ehZeXF2bOnImAgABdm//7v//DzJkzMWrUKABAp06dkJycjIULFyI6OrrK/SoUCigUigbqGZma2PibAIARXVvCwYrn9iEiMgWijQDJ5XKEhYUhLi5Ot02j0SAuLg6RkZE1PlepVMLb2xtlZWXYsWMHhg4dqnussLBQb0QIAGQyGTQaTcN2gMxC8t0CxCVpQ/qEXv7iFkNERA1G1GXw06dPR3R0NLp164bu3btj6dKlKCgoQExMDABg/Pjx8Pb2xsKFCwEAx48fR1paGkJDQ5GWlob58+dDo9Hg3Xff1e1zyJAh+OSTT+Dr64sOHTrgzJkzWLJkCSZOnChKH8m4rT+SDEEAerdxRSAvZ0FEZDJEDUAjR47EnTt3MHfuXGRkZCA0NBR79uzRTYxOSUnRG80pLi7GnDlzcP36ddja2mLQoEHYsGEDHB0ddW2WL1+ODz74AH/729+QlZUFLy8vvPHGG5g7d25Td4+MXL6qDNtPaeeDxXD0h4jIpIh6HqDmiucBIgBYF38D8/9zAQGuNvj17715pmciombOKM4DRNScaTQC1h9NBgDE9PRn+CEiMjEMQERVOHA5CzeyC2CntMALXVuKXQ4RETUwBiCiKlQsfR/ZzQc2ClGnyhERUSNgACJ6zJXMPBy6kg2pBIju6S92OURE1AgYgIges+7ITQBA/3bu8HG2FrcYIiJqFAxARI/IKSzFzoQ0AEBMr1YiV0NERI2FAYjoEVtOpqCoVI1gDzv0CHAWuxwiImokDEBE5crUGnxXvvR9Yq9WkEi49J2IyFQxABGV23chE2kPiuBsI8dfQ73ELoeIiBoRAxBRuYql769094XSUiZuMURE1KgYgIgA/JGWgxM378FCKsHYHn5il0NERI2MAYgID5e+D+zkCQ8HpbjFEBFRo2MAIrOXna/Cj4m3AfCq70RE5oIBiMzepuMpKFFrEOLjiK6+TmKXQ0RETYABiMxaSZkGG45VLH33F7cYIiJqMgxAZNb+ez4dd/JUcLNTYGBHT7HLISKiJsIARGZLEATExt8AAIzr4Qe5BX8diIjMBf/HJ7N1JvUBzt7KgVwmxegIX7HLISKiJsQARGar4sSHfw31goutQtxiiIioSTEAkVnKyCnGL+fTAXDpOxGROWIAIrO04dhNlGkEdG/ljA5eDmKXQ0RETYwBiMxOcakam46nAODSdyIic8UARGbn34lpuF9YCm9HKzzX3kPscoiISAQMQGRWtEvfbwIAonv6QSaViFsQERGJggGIzMrR63eRlJEHK0sZRnbj0nciInPFAERmZV356M8LXb3hYG0pbjFERCQaBiAyG6n3CrHvYiYALn0nIjJ3DEBkNtYfuQlBAJ4OckFrNzuxyyEiIhExAJFZKFCVYeupVADAxF6tRK6GiIjExgBEZmFHwi3kFZchwMUGvdu4il0OERGJjAGITJ5GI+gmP0f39IeUS9+JiMweAxCZvINX7uB6dgHsFBYYEdZS7HKIiKgZYAAik1cx+vNSNx/YKizELYaIiJoFBiAyaVez8nHw8h1IJMCEnv5il0NERM0EAxCZtPVHbgIA+gW7w7eFtbjFEBFRs8EARCYrp6gUOxJuAeBV34mISB8DEJmsbSdTUViiRlt3O0QGthC7HCIiakYYgMgkqTUC1h+9CUB72QuJhEvfiYjoIQYgMkn7LmTi1v0iOFpbYlgXb7HLISKiZoYBiEzSuiM3AACju/tCaSkTuRoiImpuGIDI5FxMz8Wx6/cgk0owroef2OUQEVEzxABEJic2Xjv6M6CjB7wcrUSuhoiImiMGIDIpd/NV2J14GwCXvhMRUfUYgMikbD6RgpIyDTq3dEBXXyexyyEiomaKAYhMRqlagw3HkgFw6TsREdWMAYhMxn/PpyMzVwVXOwUGd/ISuxwiImrGGIDIZMSWX/V9bIQf5Bb80SYiouo1i0+JlStXwt/fH0qlEhEREThx4kS1bUtLS7FgwQIEBgZCqVQiJCQEe/bs0Wvj7689/PH4bfLkyY3dFRLJmZT7SEx9ALlMilcifMUuh4iImjnRA9DWrVsxffp0zJs3DwkJCQgJCUFUVBSysrKqbD9nzhx8++23WL58OS5cuIA333wTw4cPx5kzZ3RtTp48ifT0dN1t3759AICXXnqpSfpETW9d+VXfnw/xhKudQtxiiIio2ZMIgiCIWUBERATCw8OxYsUKAIBGo4GPjw+mTp2KmTNnVmrv5eWF2bNn643mjBgxAlZWVti4cWOVrzFt2jT89NNPuHLlSq0mxubm5sLBwQE5OTmwt7evY8+oqWTmFqPXov0o0wj4aepT6OjtIHZJREQkAkM+v0UdASopKcHp06fRv39/3TapVIr+/fvj6NGjVT5HpVJBqVTqbbOyssLhw4erfY2NGzdi4sSJ1YYflUqF3NxcvRsZj43HklGmERDu78TwQ0REtSJqAMrOzoZarYa7u7vednd3d2RkZFT5nKioKCxZsgRXrlyBRqPBvn37sHPnTqSnp1fZfvfu3Xjw4AEmTJhQbR0LFy6Eg4OD7ubj41PnPlHTKi5VY9PxFABATK9WIldDRETGQvQ5QIZatmwZgoKCEBwcDLlcjilTpiAmJgZSadVdWbNmDQYOHAgvr+qXRc+aNQs5OTm6W2pqamOVTw3sx7O3cbegBN6OVvhLe/cnP4GIiAh1CED+/v5YsGABUlJS6v3iLi4ukMlkyMzM1NuemZkJDw+PKp/j6uqK3bt3o6CgAMnJyUhKSoKtrS0CAgIqtU1OTsavv/6K1157rcY6FAoF7O3t9W7U/AmCoFv6Pi7SDxYyo8vzREQkEoM/MaZNm4adO3ciICAAzz33HLZs2QKVSlWnF5fL5QgLC0NcXJxum0ajQVxcHCIjI2t8rlKphLe3N8rKyrBjxw4MHTq0UpvY2Fi4ublh8ODBdaqPmrcTN+7hYnoulJZSjArnYUsiIqq9OgWgxMREnDhxAu3atcPUqVPh6emJKVOmICEhweACpk+fjtWrV2P9+vW4ePEi3nrrLRQUFCAmJgYAMH78eMyaNUvX/vjx49i5cyeuX7+OQ4cOYcCAAdBoNHj33Xf19qvRaBAbG4vo6GhYWFgYXBc1fxWjP8O7tISjtVzcYoiIyKjU+ZhB165d8fXXX+P27duYN28e/vWvfyE8PByhoaFYu3Ytaru6fuTIkVi8eDHmzp2L0NBQJCYmYs+ePbqJ0SkpKXoTnIuLizFnzhy0b98ew4cPh7e3Nw4fPgxHR0e9/f76669ISUnBxIkT69pFasZS7xXifxe0E+VjeNV3IiIyUJ3PA1RaWopdu3YhNjYW+/btQ48ePfDqq6/i1q1bWLlyJfr27YtNmzY1dL1NgucBav4+/e9F/PP363iqtQs2vhYhdjlERNQMGPL5bfCxoYSEBMTGxmLz5s2QSqUYP348vvrqKwQHB+vaDB8+HOHh4YZXTlQLhSVl2HKiYum7v7jFEBGRUTI4AIWHh+O5557DqlWrMGzYMFhaWlZq06pVK4waNapBCiR63I6ENOQWl8G/hTWebesmdjlERGSEDA5A169fh5+fX41tbGxsEBsbW+eiiKqj0QhYF38DABDd0x9S6ZMvbUJERPQ4gydBZ2Vl4fjx45W2Hz9+HKdOnWqQooiqc/hqNq7dKYCtwgIvhrUUuxwiIjJSBgegyZMnV3mm5LS0NL0LlBI1htjy0Z8Xw1rCTln58CsREVFtGByALly4gK5du1ba3qVLF1y4cKFBiiKqyvU7+fjt0h1IJMCEnv5il0NEREbM4ACkUCgqXboCANLT03nCQWpU64/cBAD0besGfxcbcYshIiKjZnAA+stf/qK7eGiFBw8e4P3338dzzz3XoMURVcgtLsUPp28B4FXfiYio/gweslm8eDGeeeYZ+Pn5oUuXLgCAxMREuLu7Y8OGDQ1eIBEAbDuZioISNdq426JX6xZil0NEREbO4ADk7e2Nc+fO4fvvv8fZs2dhZWWFmJgYjB49uspzAhHVl1ojYP3RmwCACT1bQSLh0nciIqqfOk3asbGxweuvv97QtRBVKe5iJlLvFcHByhLDu3iLXQ4REZmAOs9avnDhAlJSUlBSUqK3/a9//Wu9iyJ61Lryyc+juvvASi4TtxgiIjIJdToT9PDhw3H+/HlIJBLdVd8rDkuo1eqGrZDMWlJGLo5cuwuZVILxkf5il0NERCbC4FVg77zzDlq1aoWsrCxYW1vjzz//xO+//45u3brhwIEDjVAimbN18TcBAFEd3OHtaCVuMUREZDIMHgE6evQo9u/fDxcXF0ilUkilUjz11FNYuHAh3n77bZw5c6Yx6iQzdK+gBLvOpAHg0nciImpYBo8AqdVq2NnZAQBcXFxw+/ZtAICfnx8uXbrUsNWRWdt8IgWqMg06etujm5+T2OUQEZEJMXgEqGPHjjh79ixatWqFiIgIfP7555DL5fjnP/+JgICAxqiRzFCpWoMNR5MBADFc+k5ERA3M4AA0Z84cFBQUAAAWLFiA559/Hk8//TRatGiBrVu3NniBZJ72/JGBjNxiuNjK8XyIp9jlEBGRiTE4AEVFRem+bt26NZKSknDv3j04OTnxr3RqMBVL31+J8IPCgkvfiYioYRk0B6i0tBQWFhb4448/9LY7Ozsz/FCDOXfrAU4n34elTIKxPXzFLoeIiEyQQQHI0tISvr6+PNcPNarY8qXvz3f2gpudUtxiiIjIJBm8Cmz27Nl4//33ce/evcaoh8xcVm4xfjqnXVkY08tf3GKIiMhkGTwHaMWKFbh69Sq8vLzg5+cHGxsbvccTEhIarDgyPxuPp6BULSDMzwmdWzqKXQ4REZkogwPQsGHDGqEMIkBVpsam4+VL3zn6Q0REjcjgADRv3rzGqIMI/zmbjuz8Eng6KBHVwUPscoiIyIQZPAeIqDEIgoDY+BsAgHGRfrCU8UeTiIgaj8EjQFKptMYl71whRnVxKvk+/rydC4WFFKPDufSdiIgal8EBaNeuXXr3S0tLcebMGaxfvx4ffvhhgxVG5qVi9Gd4F2842chFroaIiEydwQFo6NChlba9+OKL6NChA7Zu3YpXX321QQoj85H2oAh7/8wEAEzg5GciImoCDTbRokePHoiLi2uo3ZEZ+e7oTag1AnoGtkCwh73Y5RARkRlokABUVFSEr7/+Gt7e3g2xOzIjhSVl2HIiFQAQ06uVyNUQEZG5MPgQ2OMXPRUEAXl5ebC2tsbGjRsbtDgyfbvOpCGnqBS+ztboG+wmdjlERGQmDA5AX331lV4AkkqlcHV1RUREBJycnBq0ODJtgiBgXfl1v6J7+kMm5QV1iYioaRgcgCZMmNAIZZA5ir96F1ey8mEjl+Glbi3FLoeIiMyIwXOAYmNjsX379krbt2/fjvXr1zdIUWQeKpa+vxjWEvZKS5GrISIic2JwAFq4cCFcXFwqbXdzc8Onn37aIEWR6buZXYD9l7IAaA9/ERERNSWDA1BKSgpataq8WsfPzw8pKSkNUhSZvnVHbkIQgGfbuiLA1VbscoiIyMwYHIDc3Nxw7ty5StvPnj2LFi1aNEhRZNryikvxw+lbALj0nYiIxGFwABo9ejTefvtt/Pbbb1Cr1VCr1di/fz/eeecdjBo1qjFqJBOz/dQt5KvK0NrNFk8HVT6cSkRE1NgMXgX20Ucf4ebNm+jXrx8sLLRP12g0GD9+POcA0ROpNQLWH70JAJjQ07/GC+sSERE1FoMDkFwux9atW/Hxxx8jMTERVlZW6NSpE/z8/BqjPjIxBy5lIfluIeyVFnihK88cTkRE4jA4AFUICgpCUFBQQ9ZCZiC2/MSHo7r7wlpe5x8/IiKiejF4DtCIESPw2WefVdr++eef46WXXmqQosg0Xc7Mw+Gr2ZBKgPGRHDEkIiLxGByAfv/9dwwaNKjS9oEDB+L3339vkKLINFWM/vylvQdaOlmLWwwREZk1gwNQfn4+5HJ5pe2WlpbIzc1tkKLI9DwoLMGuMxVL3/3FLYaIiMyewQGoU6dO2Lp1a6XtW7ZsQfv27RukKDI9m0+korhUg/ae9ujeylnscoiIyMwZPAv1gw8+wAsvvIBr166hb9++AIC4uDhs2rQJP/zwQ4MXSMavTK3BhvKl7zG9uPSdiIjEZ/AI0JAhQ7B7925cvXoVf/vb3/CPf/wDaWlp2L9/P1q3bm1wAStXroS/vz+USiUiIiJw4sSJatuWlpZiwYIFCAwMhFKpREhICPbs2VOpXVpaGsaOHYsWLVrolumfOnXK4NqoYfzvQiZu5xSjhY0cQ0K8xC6HiIjI8AAEAIMHD0Z8fDwKCgpw/fp1vPzyy5gxYwZCQkIM2s/WrVsxffp0zJs3DwkJCQgJCUFUVBSysrKqbD9nzhx8++23WL58OS5cuIA333wTw4cPx5kzZ3Rt7t+/j169esHS0hK//PILLly4gC+//BJOTk516So1gIqrvr8S4QulpUzkaoiIiACJIAhCXZ74+++/Y82aNdixYwe8vLzwwgsvYMSIEQgPD6/1PiIiIhAeHo4VK1YA0J5R2sfHB1OnTsXMmTMrtffy8sLs2bMxefJk3bYRI0bAysoKGzduBADMnDkT8fHxOHToUF26BQDIzc2Fg4MDcnJyYG9vX+f9EPBHWg6eX34YFlIJ4mf2hbu9UuySiIjIRBny+W3QCFBGRgYWLVqEoKAgvPTSS7C3t4dKpcLu3buxaNEig8JPSUkJTp8+jf79+z8sRipF//79cfTo0Sqfo1KpoFTqf4BaWVnh8OHDuvs//vgjunXrhpdeeglubm7o0qULVq9eXWMtKpUKubm5ejdqGGvLR38Gd/Zk+CEiomaj1gFoyJAhaNu2Lc6dO4elS5fi9u3bWL58eZ1fODs7G2q1Gu7u7nrb3d3dkZGRUeVzoqKisGTJEly5cgUajQb79u3Dzp07kZ6ermtz/fp1rFq1CkFBQdi7dy/eeustvP3221i/fn21tSxcuBAODg66m4+PT537RQ/dyVPhp7Pa94ZXfSciouak1gHol19+wauvvooPP/wQgwcPhkzW9HM5li1bhqCgIAQHB0Mul2PKlCmIiYmBVPqwGxqNBl27dsWnn36KLl264PXXX8ekSZPwzTffVLvfWbNmIScnR3dLTU1tiu6YvO+PJ6NErUEXX0eE+jiKXQ4REZFOrQPQ4cOHkZeXh7CwMERERGDFihXIzs6u8wu7uLhAJpMhMzNTb3tmZiY8PDyqfI6rqyt2796NgoICJCcnIykpCba2tggICNC18fT0rHQ+onbt2iElJaXaWhQKBezt7fVuVD+qMjU2HtN+zzn6Q0REzU2tA1CPHj2wevVqpKen44033sCWLVvg5eWlOxSVl5dn0AvL5XKEhYUhLi5Ot02j0SAuLg6RkZE1PlepVMLb2xtlZWXYsWMHhg4dqnusV69euHTpkl77y5cv82r1Teznc+nIzlfBw16JgR2rDrRERERiMXgZvI2NDSZOnIjDhw/j/Pnz+Mc//oFFixbBzc0Nf/3rXw3a1/Tp07F69WqsX78eFy9exFtvvYWCggLExMQAAMaPH49Zs2bp2h8/fhw7d+7E9evXcejQIQwYMAAajQbvvvuurs3f//53HDt2DJ9++imuXr2KTZs24Z///KfeyjFqXIIg6K77NS7SD5ayOp1tgYiIqNHU65Opbdu2+Pzzz3Hr1i1s3rzZ4OePHDkSixcvxty5cxEaGorExETs2bNHNzE6JSVFb4JzcXEx5syZg/bt22P48OHw9vbG4cOH4ejoqGsTHh6OXbt2YfPmzejYsSM++ugjLF26FGPGjKlPV8kACSn3cT4tB3ILKUaFc0I5ERE1P3U+D5Ap43mA6mfypgT8fC4dL3dric9fNOzkmERERHXVaOcBInqS2w+KsOcP7WkMOPmZiIiaKwYgalAbjiVDrRHQI8AZ7Tw5ekZERM0TAxA1mKISNTaf4NJ3IiJq/hiAqMHsTkzDg8JS+DhboX879yc/gYiISCQMQNQgBEHAuvKl79GR/pBJJeIWREREVAMGIGoQR6/dxaXMPFjLZXipG5e+ExFR88YARA1ibfnoz4iuLeFgZSluMURERE/AAET1lny3AHFJ2mu6TejlL24xREREtcAARPW2/kgyBAHo3cYVga62YpdDRET0RAxAVC/5qjJsP5UKAIjh6A8RERkJBiCqlx9OpSJPVYYAVxs8E+QqdjlERES1wgBEdabRCFh/NBkAENPTH1IufSciIiPBAER1dvDyHdzILoCd0gIvdG0pdjlERES1xgBEdbY2/gYAYGQ3H9goLESuhoiIqPYYgKhOrmbl4dCVbEglQHRPf7HLISIiMggDENVJbPmJD/u3c4ePs7W4xRARERmIAYgMllNYip0JaQB41XciIjJODEBksC0nU1BUqkawhx16BDiLXQ4REZHBGIDIIGVqDb4rX/o+sVcrSCRc+k5ERMaHAYgM8uvFTKQ9KIKzjRx/DfUSuxwiIqI6YQAig1Rc9X10dx8oLWXiFkNERFRHDEBUa3/ezsGJG/dgIZVgXA9/scshIiKqMwYgqrWKpe8DO3nCw0EpbjFERET1wABEtZKdr8KPibcB8KrvRERk/BiAqFY2HU9BiVqDEB9HdPV1ErscIiKiemEAoicqKdNgw7GKpe/+4hZDRETUABiA6Il++SMdd/JUcLNTYGBHT7HLISIiqjcGIHqiiqXvY3v4QW7BHxkiIjJ+/DSjGiWk3MfZ1AeQy6R4JcJX7HKIiIgaBAMQ1ahi6ftfQ73gYqsQtxgiIqIGwgBE1crIKcYv59MBcOk7ERGZFgYgqtaGYzdRphHQvZUzOng5iF0OERFRg2EAoioVl6qx6XgKAC59JyIi08MARFX6MfE27heWwtvRCs+19xC7HCIiogbFAESVCIKAtfE3AADRPf0gk0pEroiIiKhhMQBRJceu30NSRh6sLGUY2Y1L34mIyPQwAFElseWjPy909YaDtaXI1RARETU8BiDSk3qvEPsuZgLg0nciIjJdDECkZ/2RmxAE4OkgF7R2sxO7HCIiokbBAEQ6BaoybD2VCgCY2KuVyNUQERE1HgYg0tmRcAt5xWUIcLFB7zauYpdDRETUaBiACACg0QhYd+QmACC6pz+kXPpOREQmjAGIAAC/X7mD63cKYKewwIiwlmKXQ0RE1KgYgAjAw6u+v9TNB7YKC3GLISIiamQMQISrWfk4ePkOJBJgQk9/scshIiJqdAxAhPXlc3/6BbvDt4W1uMUQERE1AQYgM5dTVIodCbcA8KrvRERkPhiAzNy2k6koLFGjrbsdIgNbiF0OERFRk2gWAWjlypXw9/eHUqlEREQETpw4UW3b0tJSLFiwAIGBgVAqlQgJCcGePXv02syfPx8SiUTvFhwc3NjdMDpqjYD1R28C0F72QiLh0nciIjIPogegrVu3Yvr06Zg3bx4SEhIQEhKCqKgoZGVlVdl+zpw5+Pbbb7F8+XJcuHABb775JoYPH44zZ87otevQoQPS09N1t8OHDzdFd4zKrxczcet+ERytLTE01FvscoiIiJqM6AFoyZIlmDRpEmJiYtC+fXt88803sLa2xtq1a6tsv2HDBrz//vsYNGgQAgIC8NZbb2HQoEH48ssv9dpZWFjAw8NDd3Nxcam2BpVKhdzcXL2bOai46vvo7r6wkstEroaIiKjpiBqASkpKcPr0afTv31+3TSqVon///jh69GiVz1GpVFAqlXrbrKysKo3wXLlyBV5eXggICMCYMWOQkpJSbR0LFy6Eg4OD7ubj41OPXhmHi+m5OHb9HmRSCcb18BO7HCIioiYlagDKzs6GWq2Gu7u73nZ3d3dkZGRU+ZyoqCgsWbIEV65cgUajwb59+7Bz506kp6fr2kRERGDdunXYs2cPVq1ahRs3buDpp59GXl5elfucNWsWcnJydLfU1NSG62QzVTH6M6CjB7wcrUSuhoiIqGkZ3Sl/ly1bhkmTJiE4OBgSiQSBgYGIiYnRO2Q2cOBA3dedO3dGREQE/Pz8sG3bNrz66quV9qlQKKBQKJqk/ubgbr4KuxNvA+DSdyIiMk+ijgC5uLhAJpMhMzNTb3tmZiY8PDyqfI6rqyt2796NgoICJCcnIykpCba2tggICKj2dRwdHdGmTRtcvXq1Qes3VptPpKCkTIPOLR3Q1ddJ7HKIiIianKgBSC6XIywsDHFxcbptGo0GcXFxiIyMrPG5SqUS3t7eKCsrw44dOzB06NBq2+bn5+PatWvw9PRssNqNValagw3HkgFw6TsREZkv0VeBTZ8+HatXr8b69etx8eJFvPXWWygoKEBMTAwAYPz48Zg1a5au/fHjx7Fz505cv34dhw4dwoABA6DRaPDuu+/q2syYMQMHDx7EzZs3ceTIEQwfPhwymQyjR49u8v41N7/8kYHMXBVc7RQY3MlL7HKIiIhEIfocoJEjR+LOnTuYO3cuMjIyEBoaij179ugmRqekpEAqfZjTiouLMWfOHFy/fh22trYYNGgQNmzYAEdHR12bW7duYfTo0bh79y5cXV3x1FNP4dixY3B1dW3q7jU7FZOfx0T4Qm4hev4lIiIShUQQBEHsIpqb3NxcODg4ICcnB/b29mKX02ASUx9g2Mp4yGVSxM/sC1c785n4TUREps+Qz28OAZiRitGf50M8GX6IiMisMQCZiczcYvx8TnuupIm9WolcDRERkbgYgMzExmPJKNMICPd3QkdvB7HLISIiEhUDkBkoLlVj03HtpUBiOPpDRETEAGQO/nP2Nu4WlMDLQYm/tHd/8hOIiIhMHAOQiRMEAbHxNwEA4yL9YSHjW05ERMRPQxN34sY9XEjPhdJSitHdTf8q90RERLXBAGTiKkZ/hndpCUdrubjFEBERNRMMQCYs9V4h/nchA4D2ul9ERESkxQBkwjYcS4ZGAJ5q7YI27nZil0NERNRsMACZqMKSMmw5UbH03V/cYoiIiJoZBiATtTMhDbnFZfBvYY1n27qJXQ4REVGzwgBkggRBwLojNwEA0T39IZVKxC2IiIiomWEAMkGHrmTjalY+bBUWeDGspdjlEBERNTsMQCao4qrvL4a1hJ3SUuRqiIiImh8GIBNz/U4+frt0BxIJMKGnv9jlEBERNUsMQCZmffncn75t3eDvYiNuMURERM0UA5AJyS0uxQ+nbwHgVd+JiIhqwgBkQrafuoWCEjXauNuiV+sWYpdDRETUbDEAmQi1RtAd/prQsxUkEi59JyIiqg4DkInYn5SFlHuFcLCyxPAu3mKXQ0RE1KwxAJmIiqXvo7r7wEouE7kaIiKi5o0ByAQkZeTiyLW7kEklGB/pL3Y5REREzR4DkAlYF38TABDVwR3ejlbiFkNERGQEGICM3L2CEuw6kwaAS9+JiIhqiwHIyG0+kQJVmQYdve3Rzc9J7HKIiIiMAgOQEStVa7DxWDIAIIZL34mIiGqNAciI7f0zA+k5xXCxleP5EE+xyyEiIjIaDEBGLLZ88vMrEX5QWHDpOxERUW0xABmpc7ce4HTyfVjKJBjbw1fscoiIiIwKA5CRqhj9eb6zF9zslOIWQ0REZGQYgIxQVm4xfjp3GwAQ08tf3GKIiIiMEAOQEdp4PAWlagFhfk7o3NJR7HKIiIiMDgOQkVGVqbHpePnSd47+EBER1QkDkJH56Ww6svNL4OmgRFQHD7HLISIiMkoMQEZEEATEHtFe9X1sDz9Yyvj2ERER1QU/QY3IqeT7+CMtFwoLKV7pzqXvREREdWUhdgFUe7Hx2tGf4V284WQjF7kaImqu1Go1SktLxS6DqMFZWlpCJmuYE/8yABmJtAdF2PtnJgBgAic/E1EVBEFARkYGHjx4IHYpRI3G0dERHh4e9b7+JQOQkfju6E2oNQJ6BrZAsIe92OUQUTNUEX7c3NxgbW3NCySTSREEAYWFhcjKygIAeHrW7xqYDEBGoKhEjS0nUgEAMb1aiVwNETVHarVaF35atGghdjlEjcLKygoAkJWVBTc3t3odDuMkaCOw60wacopK4etsjb7BbmKXQ0TNUMWcH2tra5ErIWpcFT/j9Z3nxgDUzAmCgHXlS9/HR/pBJuWQNhFVj4e9yNQ11M84A1AzF3/1Li5n5sNGLsPL4T5il0NERGQSGICauYql7y+GtYS90lLkaoiIjIO/vz+WLl1a6/YHDhyARCLhCjozwgDUjN3MLsD+S9rZ7tE9/cUthoioEUgkkhpv8+fPr9N+T548iddff73W7Xv27In09HQ4ODjU6fXqIjg4GAqFAhkZGU32mvQQA1Aztu7ITQgC8GxbVwS42opdDhFRg0tPT9fdli5dCnt7e71tM2bM0LUVBAFlZWW12q+rq6tBE8LlcnmDnFumtg4fPoyioiK8+OKLWL9+fZO8Zk3M8cSZzSIArVy5Ev7+/lAqlYiIiMCJEyeqbVtaWooFCxYgMDAQSqUSISEh2LNnT7XtFy1aBIlEgmnTpjVC5Y0nr7gUP5y+BYBL34mobgRBQGFJmSg3QRBqVaOHh4fu5uDgAIlEoruflJQEOzs7/PLLLwgLC4NCocDhw4dx7do1DB06FO7u7rC1tUV4eDh+/fVXvf0+fghMIpHgX//6F4YPHw5ra2sEBQXhxx9/1D3++CGwdevWwdHREXv37kW7du1ga2uLAQMGID09XfecsrIyvP3223B0dESLFi3w3nvvITo6GsOGDXtiv9esWYNXXnkF48aNw9q1ays9fuvWLYwePRrOzs6wsbFBt27dcPz4cd3j//nPfxAeHg6lUgkXFxcMHz5cr6+7d+/W25+joyPWrVsHALh58yYkEgm2bt2K3r17Q6lU4vvvv8fdu3cxevRoeHt7w9raGp06dcLmzZv19qPRaPD555+jdevWUCgU8PX1xSeffAIA6Nu3L6ZMmaLX/s6dO5DL5YiLi3vi96SpiX4eoK1bt2L69On45ptvEBERgaVLlyIqKgqXLl2Cm1vlJd9z5szBxo0bsXr1agQHB2Pv3r0YPnw4jhw5gi5duui1PXnyJL799lt07ty5qbrTYH44fQv5qjK0drPF00EuYpdDREaoqFSN9nP3ivLaFxZEwVreMB8xM2fOxOLFixEQEAAnJyekpqZi0KBB+OSTT6BQKPDdd99hyJAhuHTpEnx9q79O4ocffojPP/8cX3zxBZYvX44xY8YgOTkZzs7OVbYvLCzE4sWLsWHDBkilUowdOxYzZszA999/DwD47LPP8P333yM2Nhbt2rXDsmXLsHv3bjz77LM19icvLw/bt2/H8ePHERwcjJycHBw6dAhPP/00ACA/Px+9e/eGt7c3fvzxR3h4eCAhIQEajQYA8PPPP2P48OGYPXs2vvvuO5SUlOC///1vnb6vX375Jbp06QKlUoni4mKEhYXhvffeg729PX7++WeMGzcOgYGB6N69OwBg1qxZWL16Nb766is89dRTSE9PR1JSEgDgtddew5QpU/Dll19CoVAAADZu3Ahvb2/07dvX4Poam+gjQEuWLMGkSZMQExOD9u3b45tvvoG1tXWViRgANmzYgPfffx+DBg1CQEAA3nrrLQwaNAhffvmlXrv8/HyMGTMGq1evhpOTU401qFQq5Obm6t3EpNEIWH/kJgBgQk9/LmslIrO2YMECPPfccwgMDISzszNCQkLwxhtvoGPHjggKCsJHH32EwMBAvRGdqkyYMAGjR49G69at8emnnyI/P/+JRxy++eYbdOvWDV27dsWUKVP0RjKWL1+OWbNmYfjw4QgODsaKFSvg6Oj4xP5s2bIFQUFB6NChA2QyGUaNGoU1a9boHt+0aRPu3LmD3bt346mnnkLr1q3x8ssvIzIyEgDwySefYNSoUfjwww/Rrl07hISEYNasWU983cdNmzYNL7zwAlq1agVPT094e3tjxowZCA0NRUBAAKZOnYoBAwZg27ZtALTBbdmyZfj8888RHR2NwMBAPPXUU3jttdcAAC+88AIA4N///rfuNdatW4cJEyY0y88xUUeASkpKcPr0ab03TiqVon///jh69GiVz1GpVFAqlXrbrKyscPjwYb1tkydPxuDBg9G/f398/PHHNdaxcOFCfPjhh3XsRcP77VIWbt4thL3SAi909Ra7HCIyUlaWMlxYECXaazeUbt266d3Pz8/H/Pnz8fPPPyM9PR1lZWUoKipCSkpKjft59GiAjY0N7O3tdZdVqIq1tTUCAwN19z09PXXtc3JykJmZqRsZAQCZTIawsDDdSE111q5di7Fjx+rujx07Fr1798by5cthZ2eHxMREdOnSpdqRqcTEREyaNKnG16iNx7+varUan376KbZt24a0tDSUlJRApVLp5lJdvHgRKpUK/fr1q3J/SqVSd0jv5ZdfRkJCAv74448nBlOxiBqAsrOzoVar4e7urrfd3d1dN6T2uKioKCxZsgTPPPMMAgMDERcXh507d0KtVuvabNmyBQkJCTh58mSt6pg1axamT5+uu5+bmwsfH/HOuRMbfxMAMKq7b4MNIROR+ZFIJCbxf4iNjY3e/RkzZmDfvn1YvHgxWrduDSsrK7z44osoKSmpcT+WlvqnEpFIJDWGlara13ZuU3UuXLiAY8eO4cSJE3jvvfd029VqNbZs2YJJkybpLvdQnSc9XlWdVU1yfvz7+sUXX2DZsmVYunQpOnXqBBsbG0ybNk33fX3S6wLaw2ChoaG4desWYmNj0bdvX/j5+T3xeWIQ/RCYoZYtW4agoCAEBwdDLpdjypQpiImJgVSq7UpqaireeecdfP/995VGiqqjUChgb2+vdxPL5cw8HL6aDalEe+ZnIiLSFx8fjwkTJmD48OHo1KkTPDw8cPPmzSatwcHBAe7u7np/aKvVaiQkJNT4vDVr1uCZZ57B2bNnkZiYqLtNnz5ddxisc+fOSExMxL1796rcR+fOnWucVOzq6qo3WfvKlSsoLCx8Yp/i4+MxdOhQjB07FiEhIQgICMDly5d1jwcFBcHKyqrG1+7UqRO6deuG1atXY9OmTZg4ceITX1csogYgFxcXyGQyZGZm6m3PzMyEh4dHlc9xdXXF7t27UVBQgOTkZCQlJcHW1hYBAQEAgNOnTyMrKwtdu3aFhYUFLCwscPDgQXz99dewsLDQGylqjipGf/7S3gMtnXhNHyKixwUFBWHnzp1ITEzE2bNn8corrzzxsFNjmDp1KhYuXIh///vfuHTpEt555x3cv3+/2vkupaWl2LBhA0aPHo2OHTvq3V577TUcP34cf/75J0aPHg0PDw8MGzYM8fHxuH79Onbs2KGbGjJv3jxs3rwZ8+bNw8WLF3H+/Hl89tlnutfp27cvVqxYgTNnzuDUqVN48803K41mVSUoKAj79u3DkSNHcPHiRbzxxht6n89KpRLvvfce3n33XXz33Xe4du0ajh07pjd/CdCOAi1atAiCIOitTmtuRA1AcrkcYWFhemlSo9EgLi5ON9mrOkqlEt7e3igrK8OOHTswdOhQAEC/fv1w/vx5vWTdrVs3jBkzBomJifW6cmxje1BYgl1nKpa++4tbDBFRM7VkyRI4OTmhZ8+eGDJkCKKiotC1a9cmr+O9997D6NGjMX78eERGRsLW1hZRUVHVHn348ccfcffu3SpDQbt27dCuXTusWbMGcrkc//vf/+Dm5oZBgwahU6dOWLRoke7zq0+fPti+fTt+/PFHhIaGom/fvnqTub/88kv4+Pjg6aefxiuvvIIZM2bU6pxIc+bMQdeuXREVFYU+ffroQtijPvjgA/zjH//A3Llz0a5dO4wcObLSPKrRo0fDwsICo0ePrvWRGDFIhPoe0KynrVu3Ijo6Gt9++y26d++OpUuXYtu2bUhKSoK7uzvGjx8Pb29vLFy4EABw/PhxpKWlITQ0FGlpaZg/fz5u3LiBhISEamff9+nTB6GhobU+LXpubi4cHByQk5PTpIfDvjl4DYt+SUJ7T3v8/PZTzXLWPBE1T8XFxbhx4wZatWrVrD90TJlGo0G7du3w8ssv46OPPhK7HNHcvHkTgYGBOHnyZKME05p+1g35/BZ9dtzIkSNx584dzJ07FxkZGQgNDcWePXt0E6NTUlJ083sAbcfnzJmD69evw9bWFoMGDcKGDRtqtfSwOStTa/Bd+dL3mF5c+k5E1NwlJyfjf//7H3r37g2VSoUVK1bgxo0beOWVV8QuTRSlpaW4e/cu5syZgx49eogyKmcI0QMQAEyZMqXS2SMrHDhwQO9+7969ceHCBYP2//g+mqP/XcjE7ZxitLCRY0iIl9jlEBHRE0ilUqxbtw4zZsyAIAjo2LEjfv31V7Rr107s0kQRHx+PZ599Fm3atMEPP/wgdjlP1CwCED286vsrEb5QNuD5M4iIqHH4+PggPj5e7DKajT59+tT7NAFNyeiWwZuiP9JycPLmfVhIJRjbg0vfiYiIGhsDUDOwtnz0Z3BnT7jbc/IiERFRY2MAEtmdPBV+Oqs9YRWv+k5ERNQ0GIBEtul4CkrUGnTxdUSoj6PY5RAREZkFBiARlZRpsPF4MgCO/hARETUlBiAR/Xz+Nu7kqeBur8DAjlVf+oOIiIgaHgOQSARB0F33a1wPP1jK+FYQEdVVnz59MG3aNN19f3//J579XyKRYPfu3fV+7YbaDzUtfuqKJCHlPs7dyoHcQorR3X3FLoeISBRDhgzBgAEDqnzs0KFDkEgkOHfunMH7PXnyJF5//fX6lqdn/vz5CA0NrbQ9PT0dAwcObNDXqk5RURGcnZ3h4uIClUrVJK9pqhiARLK2fPRnWKgXWtgqxC2GiEgkr776Kvbt24dbt25Veiw2NhbdunVD586dDd6vq6trrS4A2hA8PDygUDTN/+M7duxAhw4dEBwcLPqokyAIKCsrE7WG+mAAEsHtB0XY80cGAE5+JqJGJAhASYE4t1qeEfj555+Hq6sr1q1bp7c9Pz8f27dvx6uvvoq7d+9i9OjR8Pb2hrW1NTp16oTNmzfXuN/HD4FduXIFzzzzDJRKJdq3b499+/ZVes57772HNm3awNraGgEBAfjggw9QWloKAFi3bh0+/PBDnD17FhKJBBKJRFfz44fAzp8/j759+8LKygotWrTA66+/jvz8fN3jEyZMwLBhw7B48WJ4enqiRYsWmDx5su61arJmzRqMHTsWY8eOxZo1ayo9/ueff+L555+Hvb097Ozs8PTTT+PatWu6x9euXYsOHTpAoVDA09NTdxmqmzdvQiKRIDExUdf2wYMHkEgkustJHThwABKJBL/88gvCwsKgUChw+PBhXLt2DUOHDoW7uztsbW0RHh6OX3/9Va8ulUqF9957Dz4+PlAoFGjdujXWrFkDQRDQunVrLF68WK99YmIiJBIJrl69+sTvSV3xUhgi2HAsGWqNgB4Bzmjn2XRXmyciM1NaCHwq0rUF378NyG2e2MzCwgLjx4/HunXrMHv2bN2FoLdv3w61Wo3Ro0cjPz8fYWFheO+992Bvb4+ff/4Z48aNQ2BgILp37/7E19BoNHjhhRfg7u6O48ePIycnR2++UAU7OzusW7cOXl5eOH/+PCZNmgQ7Ozu8++67GDlyJP744w/s2bNH9+Hu4OBQaR8FBQWIiopCZGQkTp48iaysLLz22muYMmWKXsj77bff4Onpid9++w1Xr17FyJEjERoaikmTJlXbj2vXruHo0aPYuXMnBEHA3//+dyQnJ8PPT3sFgbS0NDzzzDPo06cP9u/fD3t7e8THx+tGaVatWoXp06dj0aJFGDhwIHJycup0KY+ZM2di8eLFCAgIgJOTE1JTUzFo0CB88sknUCgU+O677zBkyBBcunQJvr7aKR7jx4/H0aNH8fXXXyMkJAQ3btxAdnY2JBIJJk6ciNjYWMyYMUP3GrGxsXjmmWfQunVrg+urLQagJlZcqsbmEykAOPpDRAQAEydOxBdffIGDBw+iT58+ALQfgCNGjICDgwMcHBz0PhynTp2KvXv3Ytu2bbUKQL/++iuSkpKwd+9eeHlpA+Gnn35aad7OnDlzdF/7+/tjxowZ2LJlC959911YWVnB1tYWFhYW8PCoftXupk2bUFxcjO+++w42NtoAuGLFCgwZMgSfffYZ3N3dAQBOTk5YsWIFZDIZgoODMXjwYMTFxdUYgNauXYuBAwfCyckJABAVFYXY2FjMnz8fALBy5Uo4ODhgy5YtsLS0BAC0adNG9/yPP/4Y//jHP/DOO+/otoWHhz/x+/e4BQsW4LnnntPdd3Z2RkhIiO7+Rx99hF27duHHH3/ElClTcPnyZWzbtg379u1D//79AQABAQG69hMmTMDcuXNx4sQJdO/eHaWlpdi0aVOlUaGGxgDUxHafScODwlK0dLJC/3buYpdDRKbM0lo7EiPWa9dScHAwevbsibVr16JPnz64evUqDh06hAULFgAA1Go1Pv30U2zbtg1paWkoKSmBSqWq9RyfixcvwsfHRxd+ACAyMrJSu61bt+Lrr7/GtWvXkJ+fj7KyMtjbGzZKf/HiRYSEhOjCDwD06tULGo0Gly5d0gWgDh06QCZ7eOFrT09PnD9/vtr9qtVqrF+/HsuWLdNtGzt2LGbMmIG5c+dCKpUiMTERTz/9tC78PCorKwu3b99Gv379DOpPVbp166Z3Pz8/H/Pnz8fPP/+M9PR0lJWVoaioCCkp2j/2ExMTIZPJ0Lt37yr35+XlhcGDB2Pt2rXo3r07/vOf/0ClUuGll16qd6014RygJvTo0vfoSH/IpBJxCyIi0yaRaA9DiXGTGPb/26uvvoodO3YgLy8PsbGxCAwM1H1gfvHFF1i2bBnee+89/Pbbb0hMTERUVBRKSkoa7Ft19OhRjBkzBoMGDcJPP/2EM2fOYPbs2Q36Go96PKRIJBJoNJpq2+/duxdpaWkYOXIkLCwsYGFhgVGjRiE5ORlxcXEAACsrq2qfX9NjACCVauPAo1dzr25O0qPhDgBmzJiBXbt24dNPP8WhQ4eQmJiITp066b53T3ptAHjttdewZcsWFBUVITY2FiNHjmz0SewMQE3o6LW7uJSZB2u5DC+H+4hdDhFRs/Hyyy9DKpVi06ZN+O677zBx4kTdfKD4+HgMHToUY8eORUhICAICAnD58uVa77tdu3ZITU1Fenq6btuxY8f02hw5cgR+fn6YPXs2unXrhqCgICQnJ+u1kcvlUKvVT3yts2fPoqCgQLctPj4eUqkUbdu2rXXNj1uzZg1GjRqFxMREvduoUaN0k6E7d+6MQ4cOVRlc7Ozs4O/vrwtLj3N1dQUAve/RoxOiaxIfH48JEyZg+PDh6NSpEzw8PHDz5k3d4506dYJGo8HBgwer3cegQYNgY2ODVatWYc+ePZg4cWKtXrs+GICaUEZuMeyVFhjRtSUcrCoPURIRmStbW1uMHDkSs2bNQnp6OiZMmKB7LCgoCPv27cORI0dw8eJFvPHGG8jMzKz1vvv37482bdogOjoaZ8+exaFDhzB79my9NkFBQUhJScGWLVtw7do1fP3119i1a5deG39/f9y4cQOJiYnIzs6u8jw8Y8aMgVKpRHR0NP744w/89ttvmDp1KsaNG6c7/GWoO3fu4D//+Q+io6PRsWNHvdv48eOxe/du3Lt3D1OmTEFubi5GjRqFU6dO4cqVK9iwYQMuXboEQHseoy+//BJff/01rly5goSEBCxfvhyAdpSmR48eWLRoES5evIiDBw/qzYmqSVBQEHbu3InExEScPXsWr7zyit5olr+/P6KjozFx4kTs3r0bN27cwIEDB7Bt2zZdG5lMhgkTJmDWrFkICgqq8hBlQ2MAakIvdG2JY+/3w9+fa/PkxkREZubVV1/F/fv3ERUVpTdfZ86cOejatSuioqLQp08feHh4YNiwYbXer1Qqxa5du1BUVITu3bvjtddewyeffKLX5q9//Sv+/ve/Y8qUKQgNDcWRI0fwwQcf6LUZMWIEBgwYgGeffRaurq5VLsW3trbG3r17ce/ePYSHh+PFF19Ev379sGLFCsO+GY+omFBd1fydfv36wcrKChs3bkSLFi2wf/9+5Ofno3fv3ggLC8Pq1at1h9uio6OxdOlS/L//9//QoUMHPP/887hy5YpuX2vXrkVZWRnCwsIwbdo0fPzxx7Wqb8mSJXByckLPnj0xZMgQREVFoWvXrnptVq1ahRdffBF/+9vfEBwcjEmTJumNkgHa97+kpAQxMTGGfovqRCIItTxZgxnJzc2Fg4MDcnJyDJ4AR0QkhuLiYty4cQOtWrWCUqkUuxwigx06dAj9+vVDampqjaNlNf2sG/L5zVVgREREJBqVSoU7d+5g/vz5eOmll+p8qNBQPARGREREotm8eTP8/Pzw4MEDfP755032ugxAREREJJoJEyZArVbj9OnT8Pb2brLXZQAiIiIis8MARERkQriuhUxdQ/2MMwAREZmAiqXOhYWFIldC1LgqfsaruuSHIbgKjIjIBMhkMjg6OiIrKwuA9nw0EgMvR0HUnAmCgMLCQmRlZcHR0VHvWmp1wQBERGQiKq5SXhGCiEyRo6Oj7me9PhiAiIhMhEQigaenJ9zc3Kq9kCWRMbO0tKz3yE8FBiAiIhMjk8ka7EOCyFRxEjQRERGZHQYgIiIiMjsMQERERGR2OAeoChUnWcrNzRW5EiIiIqqtis/t2pwskQGoCnl5eQAAHx8fkSshIiIiQ+Xl5cHBwaHGNhKB502vRKPR4Pbt27Czs2vwE4nl5ubCx8cHqampsLe3b9B9Nwfsn/Ez9T6aev8A0+8j+2f8GquPgiAgLy8PXl5ekEprnuXDEaAqSKVStGzZslFfw97e3mR/sAH2zxSYeh9NvX+A6feR/TN+jdHHJ438VOAkaCIiIjI7DEBERERkdhiAmphCocC8efOgUCjELqVRsH/Gz9T7aOr9A0y/j+yf8WsOfeQkaCIiIjI7HAEiIiIis8MARERERGaHAYiIiIjMDgMQERERmR0GoEawcuVK+Pv7Q6lUIiIiAidOnKix/fbt2xEcHAylUolOnTrhv//9bxNVWjeG9G/dunWQSCR6N6VS2YTVGub333/HkCFD4OXlBYlEgt27dz/xOQcOHEDXrl2hUCjQunVrrFu3rtHrrCtD+3fgwIFK759EIkFGRkbTFGyghQsXIjw8HHZ2dnBzc8OwYcNw6dKlJz7PmH4H69JHY/o9XLVqFTp37qw7QV5kZCR++eWXGp9jTO+fof0zpveuKosWLYJEIsG0adNqbCfGe8gA1MC2bt2K6dOnY968eUhISEBISAiioqKQlZVVZfsjR45g9OjRePXVV3HmzBkMGzYMw4YNwx9//NHEldeOof0DtGf6TE9P192Sk5ObsGLDFBQUICQkBCtXrqxV+xs3bmDw4MF49tlnkZiYiGnTpuG1117D3r17G7nSujG0fxUuXbqk9x66ubk1UoX1c/DgQUyePBnHjh3Dvn37UFpair/85S8oKCio9jnG9jtYlz4CxvN72LJlSyxatAinT5/GqVOn0LdvXwwdOhR//vlnle2N7f0ztH+A8bx3jzt58iS+/fZbdO7cucZ2or2HAjWo7t27C5MnT9bdV6vVgpeXl7Bw4cIq27/88svC4MGD9bZFREQIb7zxRqPWWVeG9i82NlZwcHBoouoaFgBh165dNbZ59913hQ4dOuhtGzlypBAVFdWIlTWM2vTvt99+EwAI9+/fb5KaGlpWVpYAQDh48GC1bYztd/BxtemjMf8eCoIgODk5Cf/617+qfMzY3z9BqLl/xvre5eXlCUFBQcK+ffuE3r17C++88061bcV6DzkC1IBKSkpw+vRp9O/fX7dNKpWif//+OHr0aJXPOXr0qF57AIiKiqq2vZjq0j8AyM/Ph5+fH3x8fJ74l46xMab3rz5CQ0Ph6emJ5557DvHx8WKXU2s5OTkAAGdn52rbGPt7WJs+Asb5e6hWq7FlyxYUFBQgMjKyyjbG/P7Vpn+Acb53kydPxuDBgyu9N1UR6z1kAGpA2dnZUKvVcHd319vu7u5e7ZyJjIwMg9qLqS79a9u2LdauXYt///vf2LhxIzQaDXr27Ilbt241RcmNrrr3Lzc3F0VFRSJV1XA8PT3xzTffYMeOHdixYwd8fHzQp08fJCQkiF3aE2k0GkybNg29evVCx44dq21nTL+Dj6ttH43t9/D8+fOwtbWFQqHAm2++iV27dqF9+/ZVtjXG98+Q/hnbewcAW7ZsQUJCAhYuXFir9mK9h7waPDWqyMhIvb9sevbsiXbt2uHbb7/FRx99JGJlVBtt27ZF27Ztdfd79uyJa9eu4auvvsKGDRtErOzJJk+ejD/++AOHDx8Wu5RGU9s+GtvvYdu2bZGYmIicnBz88MMPiI6OxsGDB6sNCcbGkP4Z23uXmpqKd955B/v27Wv2k7UZgBqQi4sLZDIZMjMz9bZnZmbCw8Ojyud4eHgY1F5Mdenf4ywtLdGlSxdcvXq1MUpsctW9f/b29rCyshKpqsbVvXv3Zh8qpkyZgp9++gm///47WrZsWWNbY/odfJQhfXxcc/89lMvlaN26NQAgLCwMJ0+exLJly/Dtt99WamuM758h/Xtcc3/vTp8+jaysLHTt2lW3Ta1W4/fff8eKFSugUqkgk8n0niPWe8hDYA1ILpcjLCwMcXFxum0ajQZxcXHVHt+NjIzUaw8A+/btq/F4sFjq0r/HqdVqnD9/Hp6eno1VZpMypvevoSQmJjbb908QBEyZMgW7du3C/v370apVqyc+x9jew7r08XHG9nuo0WigUqmqfMzY3r+q1NS/xzX3965fv344f/48EhMTdbdu3bphzJgxSExMrBR+ABHfw0adYm2GtmzZIigUCmHdunXChQsXhNdff11wdHQUMjIyBEEQhHHjxgkzZ87UtY+PjxcsLCyExYsXCxcvXhTmzZsnWFpaCufPnxerCzUytH8ffvihsHfvXuHatWvC6dOnhVGjRglKpVL4888/xepCjfLy8oQzZ84IZ86cEQAIS5YsEc6cOSMkJycLgiAIM2fOFMaNG6drf/36dcHa2lr4v//7P+HixYvCypUrBZlMJuzZs0esLtTI0P599dVXwu7du4UrV64I58+fF9555x1BKpUKv/76q1hdqNFbb70lODg4CAcOHBDS09N1t8LCQl0bY/8drEsfjen3cObMmcLBgweFGzduCOfOnRNmzpwpSCQS4X//+58gCMb//hnaP2N676rz+Cqw5vIeMgA1guXLlwu+vr6CXC4XunfvLhw7dkz3WO/evYXo6Gi99tu2bRPatGkjyOVyoUOHDsLPP//cxBUbxpD+TZs2TdfW3d1dGDRokJCQkCBC1bVTsez78VtFn6Kjo4XevXtXek5oaKggl8uFgIAAITY2tsnrri1D+/fZZ58JgYGBglKpFJydnYU+ffoI+/fvF6f4WqiqbwD03hNj/x2sSx+N6fdw4sSJgp+fnyCXywVXV1ehX79+unAgCMb//hnaP2N676rzeABqLu+hRBAEoXHHmIiIiIiaF84BIiIiIrPDAERERERmhwGIiIiIzA4DEBEREZkdBiAiIiIyOwxAREREZHYYgIiIiMjsMAARERGR2WEAIiKqBYlEgt27d4tdBhE1EAYgImr2JkyYAIlEUuk2YMAAsUsjIiNlIXYBRES1MWDAAMTGxuptUygUIlVDRMaOI0BEZBQUCgU8PDz0bk5OTgC0h6dWrVqFgQMHwsrKCgEBAfjhhx/0nn/+/Hn07dsXVlZWaNGiBV5//XXk5+frtVm7di06dOgAhUIBT09PTJkyRe/x7OxsDB8+HNbW1ggKCsKPP/7YuJ0mokbDAEREJuGDDz7AiBEjcPbsWYwZMwajRo3CxYsXAQAFBQWIioqCk5MTTp48ie3bt+PXX3/VCzirVq3C5MmT8frrr+P8+fP48ccf0bp1a73X+PDDD/Hyyy/j3LlzGDRoEMaMGYN79+41aT+JqIE0+vXmiYjqKTo6WpDJZIKNjY3e7ZNPPhEEQRAACG+++abecyIiIoS33npLEARB+Oc//yk4OTkJ+fn5usd//vlnQSqVChkZGYIgCIKXl5cwe/bsamsAIMyZM0d3Pz8/XwAg/PLLLw3WTyJqOpwDRERG4dlnn8WqVav0tjk7O+u+joyM1HssMjISiYmJAICLFy8iJCQENjY2usd79eoFjUaDS5cuQSKR4Pbt2+jXr1+NNXTu3Fn3tY2NDezt7ZGVlVXXLhGRiBiAiMgo2NjYVDok1VCsrKxq1c7S0lLvvkQigUajaYySiKiRcQ4QEZmEY8eOVbrfrl07AEC7du1w9uxZFBQU6B6Pj4+HVCpF27ZtYWdnB39/f8TFxTVpzUQkHo4AEZFRUKlUyMjI0NtmYWEBFxcXAMD27dvRrVs3PPXUU/j+++9x4sQJrFmzBgAwZswYzJs3D9HR0Zg/fz7u3LmDqVOnYty4cXB3dwcAzJ8/H2+++Sbc3NwwcOBA5OXlIT4+HlOnTm3ajhJRk2AAIiKjsGfPHnh6eupta9u2LZKSkgBoV2ht2bIFf/vb3+Dp6YnNmzejffv2AABra2vs3bsX77zzDsLDw2FtbY0RI0ZgyZIlun1FR0ejuLgYX331FWbMmAEXFxe8+OKLTddBImpSEkEQBLGLICKqD4lEgl27dmHYsGFil0JERoJzgIiIiMjsMAARERGR2eEcICIyejyST0SG4ggQERERmR0GICIiIjI7DEBERERkdhiAiIiIyOwwABEREZHZYQAiIiIis8MARERERGaHAYiIiIjMzv8HkYODZ/sFXDcAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Using Transformers on MNIST Data**"
      ],
      "metadata": {
        "id": "fdoREwPItlsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "NbVqbkI3tsCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is written as a tensorflow \"layer\".  it's just a vector the same size as the\n",
        "# output of the previous layer. the vector is initialized randomly, but we'll use\n",
        "# gradient descent to update the values in the vector\n",
        "#\n",
        "# it's purpose is to be appended to the beginning of the sequence of vectors fed into\n",
        "# the transformer.  then after the transformer runs on the whole data, we just grab\n",
        "# the resulting zero-th vector...the class token...and use that as the portfolio weights\n",
        "class ClassToken(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
        "            trainable = True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_dim = self.w.shape[-1]\n",
        "\n",
        "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
        "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
        "        return cls"
      ],
      "metadata": {
        "id": "qYVJHZ-Utx6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ViT(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes):\n",
        "    # n is number of rows of blocks\n",
        "    # m is number of cols of blocks\n",
        "    # block_size is number of pixels (with rgb) in each block\n",
        "\n",
        "    inp = tf.keras.layers.Input(shape=(n*m,block_size))\n",
        "    inp2 = tf.keras.layers.Input(shape=(n*m))\n",
        "    mid = tf.keras.layers.Dense(hidden_dim)(inp) # transform to vectors with different dimension\n",
        "    # the positional embeddings\n",
        "#     positions = tf.range(start=0, limit=n*m, delta=1)\n",
        "    emb = tf.keras.layers.Embedding(input_dim=n*m, output_dim=hidden_dim)(inp2) # learned positional embedding for each of the n*m possible possitions\n",
        "    mid = mid + emb # for some reason, tf.keras.layers.Add causes an error, but + doesn't?\n",
        "    # create and append class token to beginning of all input vectors\n",
        "    token = ClassToken()(mid) # append class token to beginning of sequence\n",
        "    mid = tf.keras.layers.Concatenate(axis=1)([token, mid])\n",
        "\n",
        "    for l in range(num_layers): # how many Transformer Head layers are there?\n",
        "        ln  = tf.keras.layers.LayerNormalization()(mid) # normalize\n",
        "        mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,key_dim=key_dim,value_dim=key_dim)(ln,ln,ln) # self attention!\n",
        "        add = tf.keras.layers.Add()([mid,mha]) # add and norm\n",
        "        ln  = tf.keras.layers.LayerNormalization()(add)\n",
        "        den = tf.keras.layers.Dense(mlp_dim,activation='gelu')(ln) # maybe should be relu...who knows...\n",
        "        den = tf.keras.layers.Dropout(dropout_rate)(den) # regularization\n",
        "        den = tf.keras.layers.Dense(hidden_dim)(den) # back to the right dimensional space\n",
        "        den = tf.keras.layers.Dropout(dropout_rate)(den)\n",
        "        mid = tf.keras.layers.Add()([den,add]) # add and norm again\n",
        "    ln = tf.keras.layers.LayerNormalization()(mid)\n",
        "    fl = ln[:,0,:] # just grab the class token for each image in batch\n",
        "    clas = tf.keras.layers.Dense(num_classes,activation='softmax')(fl) # probability that the image is in each category\n",
        "    mod = tf.keras.models.Model([inp,inp2],clas)\n",
        "    mod.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "    return mod"
      ],
      "metadata": {
        "id": "p8S1wKEzt0O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 7\n",
        "m = 7\n",
        "block_size = 16\n",
        "hidden_dim = 96\n",
        "num_layers = 6\n",
        "num_heads = 4\n",
        "key_dim = hidden_dim//num_heads # usually good practice for key_dim to be hidden_dim//num_heads...this is why we do Multi-Head attention\n",
        "mlp_dim = hidden_dim\n",
        "dropout_rate = 0\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "\n",
        "trans = build_ViT(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes)\n",
        "trans.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyO6NK72t2sn",
        "outputId": "77ceab0b-82e1-451c-947d-851b21d76b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 49, 16)]             0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 49)]                 0         []                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 49, 96)               1632      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 49, 96)               4704      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 49, 96)               0         ['dense[0][0]',               \n",
            " Lambda)                                                             'embedding[0][0]']           \n",
            "                                                                                                  \n",
            " class_token (ClassToken)    (None, 1, 96)                96        ['tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 50, 96)               0         ['class_token[0][0]',         \n",
            "                                                                     'tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 50, 96)               192       ['concatenate[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 50, 96)               37248     ['layer_normalization[0][0]', \n",
            " iHeadAttention)                                                     'layer_normalization[0][0]', \n",
            "                                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 50, 96)               0         ['concatenate[0][0]',         \n",
            "                                                                     'multi_head_attention[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 50, 96)               192       ['add[0][0]']                 \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 50, 96)               9312      ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 50, 96)               0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 50, 96)               9312      ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 50, 96)               0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 50, 96)               0         ['dropout_1[0][0]',           \n",
            "                                                                     'add[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 50, 96)               192       ['add_1[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 50, 96)               37248     ['layer_normalization_2[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 50, 96)               0         ['add_1[0][0]',               \n",
            "                                                                     'multi_head_attention_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 50, 96)               192       ['add_2[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 50, 96)               9312      ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 50, 96)               0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 50, 96)               9312      ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 50, 96)               0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 50, 96)               0         ['dropout_3[0][0]',           \n",
            "                                                                     'add_2[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 50, 96)               192       ['add_3[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 50, 96)               37248     ['layer_normalization_4[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 50, 96)               0         ['add_3[0][0]',               \n",
            "                                                                     'multi_head_attention_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 50, 96)               192       ['add_4[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 50, 96)               9312      ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 50, 96)               0         ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 50, 96)               9312      ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 50, 96)               0         ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 50, 96)               0         ['dropout_5[0][0]',           \n",
            "                                                                     'add_4[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 50, 96)               192       ['add_5[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (Mu  (None, 50, 96)               37248     ['layer_normalization_6[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 50, 96)               0         ['add_5[0][0]',               \n",
            "                                                                     'multi_head_attention_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 50, 96)               192       ['add_6[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 50, 96)               9312      ['layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 50, 96)               0         ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 50, 96)               9312      ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 50, 96)               0         ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 50, 96)               0         ['dropout_7[0][0]',           \n",
            "                                                                     'add_6[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_8 (Lay  (None, 50, 96)               192       ['add_7[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (Mu  (None, 50, 96)               37248     ['layer_normalization_8[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_8[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 50, 96)               0         ['add_7[0][0]',               \n",
            "                                                                     'multi_head_attention_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_9 (Lay  (None, 50, 96)               192       ['add_8[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 50, 96)               9312      ['layer_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)         (None, 50, 96)               0         ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 50, 96)               9312      ['dropout_8[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)         (None, 50, 96)               0         ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 50, 96)               0         ['dropout_9[0][0]',           \n",
            "                                                                     'add_8[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_10 (La  (None, 50, 96)               192       ['add_9[0][0]']               \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (Mu  (None, 50, 96)               37248     ['layer_normalization_10[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_10[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 50, 96)               0         ['add_9[0][0]',               \n",
            "                                                                     'multi_head_attention_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_11 (La  (None, 50, 96)               192       ['add_10[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 50, 96)               9312      ['layer_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 50, 96)               0         ['dense_11[0][0]']            \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 50, 96)               9312      ['dropout_10[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)        (None, 50, 96)               0         ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 50, 96)               0         ['dropout_11[0][0]',          \n",
            "                                                                     'add_10[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_12 (La  (None, 50, 96)               192       ['add_11[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 96)                   0         ['layer_normalization_12[0][0]\n",
            " SlicingOpLambda)                                                   ']                            \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 10)                   970       ['tf.__operators__.getitem[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 345130 (1.32 MB)\n",
            "Trainable params: 345130 (1.32 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "ndata_train = x_train.shape[0]\n",
        "ndata_test = x_test.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-ynpXhVt5LL",
        "outputId": "1edce64d-ec27-41b2-ad17-4aba9d2cb3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUoSLaQTt8dO",
        "outputId": "0b1553d3-6798-497d-db59-e81c45789775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_ravel = np.zeros((ndata_train,n*m,block_size))\n",
        "for img in range(ndata_train):\n",
        "    ind = 0\n",
        "    for row in range(n):\n",
        "        for col in range(m):\n",
        "            x_train_ravel[img,ind,:] = x_train[img,(row*4):((row+1)*4),(col*4):((col+1)*4)].ravel()\n",
        "            ind += 1\n"
      ],
      "metadata": {
        "id": "JhAsoGe1t_C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_ravel = np.zeros((ndata_test,n*m,block_size))\n",
        "for img in range(ndata_test):\n",
        "    ind = 0\n",
        "    for row in range(n):\n",
        "        for col in range(m):\n",
        "            x_test_ravel[img,ind,:] = x_test[img,(row*4):((row+1)*4),(col*4):((col+1)*4)].ravel()\n",
        "            ind += 1"
      ],
      "metadata": {
        "id": "CP7yex-_uBni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_feed_train = np.array([list(range(n*m))]*ndata_train)\n",
        "pos_feed_test = np.array([list(range(n*m))]*ndata_test)"
      ],
      "metadata": {
        "id": "c3UGvwCjuJxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans.fit([x_train_ravel,pos_feed_train],y_train,epochs=50,batch_size = 40,validation_split=0.20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7cb0NXruLuR",
        "outputId": "61a51960-7e92-49f8-b9ed-55707ad3c4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1200/1200 [==============================] - 40s 21ms/step - loss: 0.5805 - accuracy: 0.8023 - val_loss: 0.3174 - val_accuracy: 0.9004\n",
            "Epoch 2/50\n",
            "1200/1200 [==============================] - 26s 22ms/step - loss: 0.2233 - accuracy: 0.9284 - val_loss: 0.1921 - val_accuracy: 0.9399\n",
            "Epoch 3/50\n",
            "1200/1200 [==============================] - 24s 20ms/step - loss: 0.1662 - accuracy: 0.9478 - val_loss: 0.1237 - val_accuracy: 0.9620\n",
            "Epoch 4/50\n",
            "1200/1200 [==============================] - 26s 22ms/step - loss: 0.1353 - accuracy: 0.9566 - val_loss: 0.1521 - val_accuracy: 0.9505\n",
            "Epoch 5/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.1184 - accuracy: 0.9621 - val_loss: 0.0987 - val_accuracy: 0.9688\n",
            "Epoch 6/50\n",
            "1200/1200 [==============================] - 24s 20ms/step - loss: 0.0993 - accuracy: 0.9681 - val_loss: 0.0927 - val_accuracy: 0.9709\n",
            "Epoch 7/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0883 - accuracy: 0.9720 - val_loss: 0.1012 - val_accuracy: 0.9676\n",
            "Epoch 8/50\n",
            "1200/1200 [==============================] - 25s 20ms/step - loss: 0.0788 - accuracy: 0.9742 - val_loss: 0.0867 - val_accuracy: 0.9720\n",
            "Epoch 9/50\n",
            "1200/1200 [==============================] - 24s 20ms/step - loss: 0.0733 - accuracy: 0.9766 - val_loss: 0.0819 - val_accuracy: 0.9753\n",
            "Epoch 10/50\n",
            "1200/1200 [==============================] - 25s 20ms/step - loss: 0.0632 - accuracy: 0.9793 - val_loss: 0.0668 - val_accuracy: 0.9805\n",
            "Epoch 11/50\n",
            "1200/1200 [==============================] - 25s 20ms/step - loss: 0.0586 - accuracy: 0.9809 - val_loss: 0.0928 - val_accuracy: 0.9717\n",
            "Epoch 12/50\n",
            "1200/1200 [==============================] - 24s 20ms/step - loss: 0.0555 - accuracy: 0.9816 - val_loss: 0.0961 - val_accuracy: 0.9697\n",
            "Epoch 13/50\n",
            "1200/1200 [==============================] - 24s 20ms/step - loss: 0.0496 - accuracy: 0.9835 - val_loss: 0.0766 - val_accuracy: 0.9772\n",
            "Epoch 14/50\n",
            "1200/1200 [==============================] - 24s 20ms/step - loss: 0.0456 - accuracy: 0.9849 - val_loss: 0.0776 - val_accuracy: 0.9758\n",
            "Epoch 15/50\n",
            "1200/1200 [==============================] - 25s 20ms/step - loss: 0.0436 - accuracy: 0.9858 - val_loss: 0.0673 - val_accuracy: 0.9804\n",
            "Epoch 16/50\n",
            "1200/1200 [==============================] - 26s 21ms/step - loss: 0.0406 - accuracy: 0.9864 - val_loss: 0.0739 - val_accuracy: 0.9768\n",
            "Epoch 17/50\n",
            "1200/1200 [==============================] - 26s 22ms/step - loss: 0.0379 - accuracy: 0.9876 - val_loss: 0.0682 - val_accuracy: 0.9793\n",
            "Epoch 18/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0367 - accuracy: 0.9874 - val_loss: 0.0700 - val_accuracy: 0.9778\n",
            "Epoch 19/50\n",
            "1200/1200 [==============================] - 26s 21ms/step - loss: 0.0333 - accuracy: 0.9889 - val_loss: 0.0572 - val_accuracy: 0.9836\n",
            "Epoch 20/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0312 - accuracy: 0.9897 - val_loss: 0.0652 - val_accuracy: 0.9806\n",
            "Epoch 21/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0295 - accuracy: 0.9901 - val_loss: 0.0844 - val_accuracy: 0.9774\n",
            "Epoch 22/50\n",
            "1200/1200 [==============================] - 26s 22ms/step - loss: 0.0294 - accuracy: 0.9901 - val_loss: 0.0638 - val_accuracy: 0.9795\n",
            "Epoch 23/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0256 - accuracy: 0.9909 - val_loss: 0.0546 - val_accuracy: 0.9847\n",
            "Epoch 24/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0261 - accuracy: 0.9904 - val_loss: 0.0585 - val_accuracy: 0.9817\n",
            "Epoch 25/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0237 - accuracy: 0.9920 - val_loss: 0.0637 - val_accuracy: 0.9808\n",
            "Epoch 26/50\n",
            "1200/1200 [==============================] - 26s 22ms/step - loss: 0.0257 - accuracy: 0.9913 - val_loss: 0.0619 - val_accuracy: 0.9826\n",
            "Epoch 27/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0211 - accuracy: 0.9925 - val_loss: 0.0575 - val_accuracy: 0.9843\n",
            "Epoch 28/50\n",
            "1200/1200 [==============================] - 26s 22ms/step - loss: 0.0238 - accuracy: 0.9917 - val_loss: 0.0549 - val_accuracy: 0.9835\n",
            "Epoch 29/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0208 - accuracy: 0.9929 - val_loss: 0.0710 - val_accuracy: 0.9803\n",
            "Epoch 30/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0200 - accuracy: 0.9934 - val_loss: 0.0573 - val_accuracy: 0.9835\n",
            "Epoch 31/50\n",
            "1200/1200 [==============================] - 26s 21ms/step - loss: 0.0197 - accuracy: 0.9931 - val_loss: 0.0551 - val_accuracy: 0.9838\n",
            "Epoch 32/50\n",
            "1200/1200 [==============================] - 26s 22ms/step - loss: 0.0186 - accuracy: 0.9935 - val_loss: 0.0579 - val_accuracy: 0.9847\n",
            "Epoch 33/50\n",
            "1200/1200 [==============================] - 26s 21ms/step - loss: 0.0168 - accuracy: 0.9943 - val_loss: 0.0599 - val_accuracy: 0.9843\n",
            "Epoch 34/50\n",
            "1200/1200 [==============================] - 27s 22ms/step - loss: 0.0190 - accuracy: 0.9933 - val_loss: 0.0677 - val_accuracy: 0.9822\n",
            "Epoch 35/50\n",
            "1200/1200 [==============================] - 26s 22ms/step - loss: 0.0163 - accuracy: 0.9945 - val_loss: 0.0626 - val_accuracy: 0.9828\n",
            "Epoch 36/50\n",
            "1200/1200 [==============================] - 26s 22ms/step - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.0556 - val_accuracy: 0.9846\n",
            "Epoch 37/50\n",
            "1200/1200 [==============================] - 26s 21ms/step - loss: 0.0178 - accuracy: 0.9937 - val_loss: 0.0611 - val_accuracy: 0.9843\n",
            "Epoch 38/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0144 - accuracy: 0.9950 - val_loss: 0.0638 - val_accuracy: 0.9827\n",
            "Epoch 39/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0147 - accuracy: 0.9949 - val_loss: 0.0777 - val_accuracy: 0.9802\n",
            "Epoch 40/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0135 - accuracy: 0.9950 - val_loss: 0.0588 - val_accuracy: 0.9833\n",
            "Epoch 41/50\n",
            "1200/1200 [==============================] - 26s 21ms/step - loss: 0.0142 - accuracy: 0.9949 - val_loss: 0.0677 - val_accuracy: 0.9824\n",
            "Epoch 42/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0607 - val_accuracy: 0.9858\n",
            "Epoch 43/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0139 - accuracy: 0.9958 - val_loss: 0.0534 - val_accuracy: 0.9873\n",
            "Epoch 44/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0569 - val_accuracy: 0.9839\n",
            "Epoch 45/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0129 - accuracy: 0.9956 - val_loss: 0.0598 - val_accuracy: 0.9843\n",
            "Epoch 46/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0123 - accuracy: 0.9959 - val_loss: 0.0637 - val_accuracy: 0.9843\n",
            "Epoch 47/50\n",
            "1200/1200 [==============================] - 25s 20ms/step - loss: 0.0125 - accuracy: 0.9957 - val_loss: 0.0745 - val_accuracy: 0.9828\n",
            "Epoch 48/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0118 - accuracy: 0.9959 - val_loss: 0.0693 - val_accuracy: 0.9818\n",
            "Epoch 49/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 0.0491 - val_accuracy: 0.9867\n",
            "Epoch 50/50\n",
            "1200/1200 [==============================] - 25s 21ms/step - loss: 0.0123 - accuracy: 0.9956 - val_loss: 0.0657 - val_accuracy: 0.9841\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d4ce776acb0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = trans.evaluate([x_test_ravel,pos_feed_test],y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srzuhSrtuOXh",
        "outputId": "e7e84aeb-bb87-41f8-f4db-9d4903b69444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 6s 17ms/step - loss: 0.0632 - accuracy: 0.9841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 7\n",
        "m = 7\n",
        "block_size = 16\n",
        "hidden_dim = 64\n",
        "num_layers = 6\n",
        "num_heads = 6\n",
        "key_dim = hidden_dim//num_heads # usually good practice for key_dim to be hidden_dim//num_heads...this is why we do Multi-Head attention\n",
        "mlp_dim = hidden_dim\n",
        "dropout_rate = 0.10\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "\n",
        "trans = build_ViT(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes)\n",
        "trans.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed734d7-790d-4f1f-f1e9-d29df8f76ea2",
        "id": "aavqYvay-0oe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)        [(None, 49, 16)]             0         []                            \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)        [(None, 49)]                 0         []                            \n",
            "                                                                                                  \n",
            " dense_28 (Dense)            (None, 49, 64)               1088      ['input_5[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)     (None, 49, 64)               3136      ['input_6[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TF  (None, 49, 64)               0         ['dense_28[0][0]',            \n",
            " OpLambda)                                                           'embedding_2[0][0]']         \n",
            "                                                                                                  \n",
            " class_token_2 (ClassToken)  (None, 1, 64)                64        ['tf.__operators__.add_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 50, 64)               0         ['class_token_2[0][0]',       \n",
            " )                                                                   'tf.__operators__.add_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_26 (La  (None, 50, 64)               128       ['concatenate_2[0][0]']       \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_12 (M  (None, 50, 64)               15604     ['layer_normalization_26[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_26[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_26[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_24 (Add)                (None, 50, 64)               0         ['concatenate_2[0][0]',       \n",
            "                                                                     'multi_head_attention_12[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_27 (La  (None, 50, 64)               128       ['add_24[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_29 (Dense)            (None, 50, 64)               4160      ['layer_normalization_27[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)        (None, 50, 64)               0         ['dense_29[0][0]']            \n",
            "                                                                                                  \n",
            " dense_30 (Dense)            (None, 50, 64)               4160      ['dropout_24[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)        (None, 50, 64)               0         ['dense_30[0][0]']            \n",
            "                                                                                                  \n",
            " add_25 (Add)                (None, 50, 64)               0         ['dropout_25[0][0]',          \n",
            "                                                                     'add_24[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_28 (La  (None, 50, 64)               128       ['add_25[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_13 (M  (None, 50, 64)               15604     ['layer_normalization_28[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_28[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_28[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_26 (Add)                (None, 50, 64)               0         ['add_25[0][0]',              \n",
            "                                                                     'multi_head_attention_13[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_29 (La  (None, 50, 64)               128       ['add_26[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_31 (Dense)            (None, 50, 64)               4160      ['layer_normalization_29[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)        (None, 50, 64)               0         ['dense_31[0][0]']            \n",
            "                                                                                                  \n",
            " dense_32 (Dense)            (None, 50, 64)               4160      ['dropout_26[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)        (None, 50, 64)               0         ['dense_32[0][0]']            \n",
            "                                                                                                  \n",
            " add_27 (Add)                (None, 50, 64)               0         ['dropout_27[0][0]',          \n",
            "                                                                     'add_26[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_30 (La  (None, 50, 64)               128       ['add_27[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_14 (M  (None, 50, 64)               15604     ['layer_normalization_30[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_30[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_30[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_28 (Add)                (None, 50, 64)               0         ['add_27[0][0]',              \n",
            "                                                                     'multi_head_attention_14[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_31 (La  (None, 50, 64)               128       ['add_28[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_33 (Dense)            (None, 50, 64)               4160      ['layer_normalization_31[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)        (None, 50, 64)               0         ['dense_33[0][0]']            \n",
            "                                                                                                  \n",
            " dense_34 (Dense)            (None, 50, 64)               4160      ['dropout_28[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)        (None, 50, 64)               0         ['dense_34[0][0]']            \n",
            "                                                                                                  \n",
            " add_29 (Add)                (None, 50, 64)               0         ['dropout_29[0][0]',          \n",
            "                                                                     'add_28[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_32 (La  (None, 50, 64)               128       ['add_29[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_15 (M  (None, 50, 64)               15604     ['layer_normalization_32[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_32[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_32[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_30 (Add)                (None, 50, 64)               0         ['add_29[0][0]',              \n",
            "                                                                     'multi_head_attention_15[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_33 (La  (None, 50, 64)               128       ['add_30[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_35 (Dense)            (None, 50, 64)               4160      ['layer_normalization_33[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)        (None, 50, 64)               0         ['dense_35[0][0]']            \n",
            "                                                                                                  \n",
            " dense_36 (Dense)            (None, 50, 64)               4160      ['dropout_30[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_31 (Dropout)        (None, 50, 64)               0         ['dense_36[0][0]']            \n",
            "                                                                                                  \n",
            " add_31 (Add)                (None, 50, 64)               0         ['dropout_31[0][0]',          \n",
            "                                                                     'add_30[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_34 (La  (None, 50, 64)               128       ['add_31[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_16 (M  (None, 50, 64)               15604     ['layer_normalization_34[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_34[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_34[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_32 (Add)                (None, 50, 64)               0         ['add_31[0][0]',              \n",
            "                                                                     'multi_head_attention_16[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_35 (La  (None, 50, 64)               128       ['add_32[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_37 (Dense)            (None, 50, 64)               4160      ['layer_normalization_35[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_32 (Dropout)        (None, 50, 64)               0         ['dense_37[0][0]']            \n",
            "                                                                                                  \n",
            " dense_38 (Dense)            (None, 50, 64)               4160      ['dropout_32[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_33 (Dropout)        (None, 50, 64)               0         ['dense_38[0][0]']            \n",
            "                                                                                                  \n",
            " add_33 (Add)                (None, 50, 64)               0         ['dropout_33[0][0]',          \n",
            "                                                                     'add_32[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_36 (La  (None, 50, 64)               128       ['add_33[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_17 (M  (None, 50, 64)               15604     ['layer_normalization_36[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_36[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_36[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_34 (Add)                (None, 50, 64)               0         ['add_33[0][0]',              \n",
            "                                                                     'multi_head_attention_17[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_37 (La  (None, 50, 64)               128       ['add_34[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_39 (Dense)            (None, 50, 64)               4160      ['layer_normalization_37[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_34 (Dropout)        (None, 50, 64)               0         ['dense_39[0][0]']            \n",
            "                                                                                                  \n",
            " dense_40 (Dense)            (None, 50, 64)               4160      ['dropout_34[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_35 (Dropout)        (None, 50, 64)               0         ['dense_40[0][0]']            \n",
            "                                                                                                  \n",
            " add_35 (Add)                (None, 50, 64)               0         ['dropout_35[0][0]',          \n",
            "                                                                     'add_34[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_38 (La  (None, 50, 64)               128       ['add_35[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 64)                   0         ['layer_normalization_38[0][0]\n",
            "  (SlicingOpLambda)                                                 ']                            \n",
            "                                                                                                  \n",
            " dense_41 (Dense)            (None, 10)                   650       ['tf.__operators__.getitem_2[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 150146 (586.51 KB)\n",
            "Trainable params: 150146 (586.51 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Train the model\n",
        "trans.fit([x_train_ravel, pos_feed_train], y_train, epochs=100, batch_size=40, validation_split=0.20,\n",
        "                    callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ois5cdOMVUI",
        "outputId": "948706d9-e854-4611-dc10-19f3b48b1343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1200/1200 [==============================] - 332s 268ms/step - loss: 0.6503 - accuracy: 0.7814 - val_loss: 0.2484 - val_accuracy: 0.9276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100\n",
            "1200/1200 [==============================] - 331s 276ms/step - loss: 0.2735 - accuracy: 0.9142 - val_loss: 0.1772 - val_accuracy: 0.9477\n",
            "Epoch 3/100\n",
            "1200/1200 [==============================] - 337s 281ms/step - loss: 0.2042 - accuracy: 0.9360 - val_loss: 0.2016 - val_accuracy: 0.9377\n",
            "Epoch 4/100\n",
            "1200/1200 [==============================] - 340s 283ms/step - loss: 0.1659 - accuracy: 0.9486 - val_loss: 0.1371 - val_accuracy: 0.9589\n",
            "Epoch 5/100\n",
            "1200/1200 [==============================] - 336s 280ms/step - loss: 0.1431 - accuracy: 0.9553 - val_loss: 0.1306 - val_accuracy: 0.9615\n",
            "Epoch 6/100\n",
            "1200/1200 [==============================] - 338s 281ms/step - loss: 0.1297 - accuracy: 0.9593 - val_loss: 0.1229 - val_accuracy: 0.9627\n",
            "Epoch 7/100\n",
            "1200/1200 [==============================] - 323s 269ms/step - loss: 0.1186 - accuracy: 0.9626 - val_loss: 0.1086 - val_accuracy: 0.9679\n",
            "Epoch 8/100\n",
            "1200/1200 [==============================] - 337s 281ms/step - loss: 0.1099 - accuracy: 0.9651 - val_loss: 0.1416 - val_accuracy: 0.9582\n",
            "Epoch 9/100\n",
            "1200/1200 [==============================] - 336s 280ms/step - loss: 0.0998 - accuracy: 0.9685 - val_loss: 0.0997 - val_accuracy: 0.9717\n",
            "Epoch 10/100\n",
            "1200/1200 [==============================] - 325s 271ms/step - loss: 0.0934 - accuracy: 0.9706 - val_loss: 0.0856 - val_accuracy: 0.9744\n",
            "Epoch 11/100\n",
            "1200/1200 [==============================] - 331s 276ms/step - loss: 0.0836 - accuracy: 0.9730 - val_loss: 0.0982 - val_accuracy: 0.9712\n",
            "Epoch 12/100\n",
            "1200/1200 [==============================] - 342s 285ms/step - loss: 0.0833 - accuracy: 0.9729 - val_loss: 0.1001 - val_accuracy: 0.9710\n",
            "Epoch 13/100\n",
            "1200/1200 [==============================] - 345s 288ms/step - loss: 0.0799 - accuracy: 0.9743 - val_loss: 0.0798 - val_accuracy: 0.9768\n",
            "Epoch 14/100\n",
            "1200/1200 [==============================] - 341s 284ms/step - loss: 0.0760 - accuracy: 0.9755 - val_loss: 0.0988 - val_accuracy: 0.9691\n",
            "Epoch 15/100\n",
            "1200/1200 [==============================] - 338s 282ms/step - loss: 0.0684 - accuracy: 0.9778 - val_loss: 0.0803 - val_accuracy: 0.9769\n",
            "Epoch 16/100\n",
            "1200/1200 [==============================] - 332s 277ms/step - loss: 0.0693 - accuracy: 0.9775 - val_loss: 0.0860 - val_accuracy: 0.9753\n",
            "Epoch 17/100\n",
            "1200/1200 [==============================] - 342s 285ms/step - loss: 0.0640 - accuracy: 0.9789 - val_loss: 0.0810 - val_accuracy: 0.9747\n",
            "Epoch 18/100\n",
            "1200/1200 [==============================] - 325s 271ms/step - loss: 0.0576 - accuracy: 0.9809 - val_loss: 0.0658 - val_accuracy: 0.9801\n",
            "Epoch 19/100\n",
            "1200/1200 [==============================] - 324s 270ms/step - loss: 0.0579 - accuracy: 0.9815 - val_loss: 0.0914 - val_accuracy: 0.9724\n",
            "Epoch 20/100\n",
            "1200/1200 [==============================] - 342s 285ms/step - loss: 0.0568 - accuracy: 0.9814 - val_loss: 0.0786 - val_accuracy: 0.9777\n",
            "Epoch 21/100\n",
            "1200/1200 [==============================] - 340s 283ms/step - loss: 0.0527 - accuracy: 0.9826 - val_loss: 0.0769 - val_accuracy: 0.9783\n",
            "Epoch 22/100\n",
            "1200/1200 [==============================] - 344s 286ms/step - loss: 0.0520 - accuracy: 0.9831 - val_loss: 0.0640 - val_accuracy: 0.9803\n",
            "Epoch 23/100\n",
            "1200/1200 [==============================] - 343s 286ms/step - loss: 0.0474 - accuracy: 0.9845 - val_loss: 0.0765 - val_accuracy: 0.9781\n",
            "Epoch 24/100\n",
            "1200/1200 [==============================] - 346s 289ms/step - loss: 0.0476 - accuracy: 0.9837 - val_loss: 0.0658 - val_accuracy: 0.9803\n",
            "Epoch 25/100\n",
            "1200/1200 [==============================] - 344s 287ms/step - loss: 0.0437 - accuracy: 0.9848 - val_loss: 0.0717 - val_accuracy: 0.9808\n",
            "Epoch 26/100\n",
            "1200/1200 [==============================] - 348s 290ms/step - loss: 0.0436 - accuracy: 0.9857 - val_loss: 0.0759 - val_accuracy: 0.9784\n",
            "Epoch 27/100\n",
            "1200/1200 [==============================] - 346s 288ms/step - loss: 0.0404 - accuracy: 0.9865 - val_loss: 0.0688 - val_accuracy: 0.9803\n",
            "Epoch 28/100\n",
            "1200/1200 [==============================] - 343s 285ms/step - loss: 0.0425 - accuracy: 0.9861 - val_loss: 0.0752 - val_accuracy: 0.9793\n",
            "Epoch 29/100\n",
            "1200/1200 [==============================] - 347s 290ms/step - loss: 0.0384 - accuracy: 0.9875 - val_loss: 0.0713 - val_accuracy: 0.9818\n",
            "Epoch 30/100\n",
            "1200/1200 [==============================] - 345s 287ms/step - loss: 0.0397 - accuracy: 0.9869 - val_loss: 0.0693 - val_accuracy: 0.9808\n",
            "Epoch 31/100\n",
            "1200/1200 [==============================] - 344s 287ms/step - loss: 0.0383 - accuracy: 0.9873 - val_loss: 0.0822 - val_accuracy: 0.9769\n",
            "Epoch 32/100\n",
            "1200/1200 [==============================] - 335s 279ms/step - loss: 0.0338 - accuracy: 0.9882 - val_loss: 0.0616 - val_accuracy: 0.9833\n",
            "Epoch 33/100\n",
            "1200/1200 [==============================] - 348s 290ms/step - loss: 0.0361 - accuracy: 0.9878 - val_loss: 0.0711 - val_accuracy: 0.9809\n",
            "Epoch 34/100\n",
            "1200/1200 [==============================] - 343s 286ms/step - loss: 0.0353 - accuracy: 0.9880 - val_loss: 0.0719 - val_accuracy: 0.9815\n",
            "Epoch 35/100\n",
            "1200/1200 [==============================] - 343s 286ms/step - loss: 0.0331 - accuracy: 0.9884 - val_loss: 0.0734 - val_accuracy: 0.9812\n",
            "Epoch 36/100\n",
            "1200/1200 [==============================] - 343s 286ms/step - loss: 0.0305 - accuracy: 0.9895 - val_loss: 0.0645 - val_accuracy: 0.9825\n",
            "Epoch 37/100\n",
            "1200/1200 [==============================] - 344s 287ms/step - loss: 0.0320 - accuracy: 0.9893 - val_loss: 0.0664 - val_accuracy: 0.9816\n",
            "Epoch 38/100\n",
            "1200/1200 [==============================] - 342s 285ms/step - loss: 0.0307 - accuracy: 0.9893 - val_loss: 0.0631 - val_accuracy: 0.9824\n",
            "Epoch 39/100\n",
            "1200/1200 [==============================] - 341s 284ms/step - loss: 0.0299 - accuracy: 0.9899 - val_loss: 0.0668 - val_accuracy: 0.9822\n",
            "Epoch 40/100\n",
            "1200/1200 [==============================] - 344s 286ms/step - loss: 0.0292 - accuracy: 0.9906 - val_loss: 0.0691 - val_accuracy: 0.9819\n",
            "Epoch 41/100\n",
            "1200/1200 [==============================] - 346s 289ms/step - loss: 0.0295 - accuracy: 0.9903 - val_loss: 0.0600 - val_accuracy: 0.9847\n",
            "Epoch 42/100\n",
            "1200/1200 [==============================] - 345s 288ms/step - loss: 0.0286 - accuracy: 0.9901 - val_loss: 0.0658 - val_accuracy: 0.9810\n",
            "Epoch 43/100\n",
            "1200/1200 [==============================] - 332s 276ms/step - loss: 0.0259 - accuracy: 0.9915 - val_loss: 0.0838 - val_accuracy: 0.9776\n",
            "Epoch 44/100\n",
            "1200/1200 [==============================] - 346s 288ms/step - loss: 0.0254 - accuracy: 0.9908 - val_loss: 0.0783 - val_accuracy: 0.9803\n",
            "Epoch 45/100\n",
            "1200/1200 [==============================] - 345s 288ms/step - loss: 0.0281 - accuracy: 0.9903 - val_loss: 0.0634 - val_accuracy: 0.9832\n",
            "Epoch 46/100\n",
            "1200/1200 [==============================] - 337s 281ms/step - loss: 0.0227 - accuracy: 0.9919 - val_loss: 0.0606 - val_accuracy: 0.9837\n",
            "Epoch 47/100\n",
            "1200/1200 [==============================] - 348s 290ms/step - loss: 0.0231 - accuracy: 0.9919 - val_loss: 0.0672 - val_accuracy: 0.9827\n",
            "Epoch 48/100\n",
            "1200/1200 [==============================] - 346s 289ms/step - loss: 0.0269 - accuracy: 0.9906 - val_loss: 0.0694 - val_accuracy: 0.9812\n",
            "Epoch 49/100\n",
            "1200/1200 [==============================] - 344s 287ms/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.0722 - val_accuracy: 0.9812\n",
            "Epoch 50/100\n",
            "1200/1200 [==============================] - 344s 287ms/step - loss: 0.0212 - accuracy: 0.9924 - val_loss: 0.0647 - val_accuracy: 0.9836\n",
            "Epoch 51/100\n",
            "1200/1200 [==============================] - 347s 289ms/step - loss: 0.0230 - accuracy: 0.9921 - val_loss: 0.0634 - val_accuracy: 0.9827\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b2f18bb94b0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = trans.evaluate([x_test_ravel,pos_feed_test],y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "8a4bb427-aa44-49c9-b93d-810d1ed13f1a",
        "id": "SUAPBu_GXTFr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trans' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9733551be806>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test_ravel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_feed_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trans' is not defined"
          ]
        }
      ]
    }
  ]
}